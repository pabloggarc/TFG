@article{Gao2010,
   abstract = {This paper proposes a method which combines Sobel edge detection operator and soft-threshold wavelet de-noising to do edge detection on images which include White Gaussian noises. In recent years, a lot of edge detection methods are proposed. The commonly used methods which combine mean de-noising and Sobel operator or median filtering and Sobel operator can not remove salt and pepper noise very well. In this paper, we firstly use soft-threshold wavelet to remove noise, then use Sobel edge detection operator to do edge detection on the image. This method is mainly used on the images which includes White Gaussian noises. Through the pictures obtained by the experiment, we can see very clearly that, compared to the traditional edge detection methods, the method proposed in this paper has a more obvious effect on edge detection. © 2010 IEEE.},
   author = {Wenshuo Gao and Lei Yang and Xiaoguang Zhang and Huizhong Liu},
   doi = {10.1109/ICCSIT.2010.5563693},
   isbn = {9781424455386},
   journal = {Proceedings - 2010 3rd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2010},
   keywords = {Edge detection,Edge detection operator,Soft-threshold wavelet de-noising,White Gaussian noises},
   pages = {67-71},
   title = {An improved Sobel edge detection},
   volume = {5},
   year = {2010},
}
@book{Goodfellow-et-al-2016,
   author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
   pages = {326-330},
   publisher = {MIT Press},
   title = {Deep Learning},
   url = {http://www.deeplearningbook.org},
   year = {2016},
}
@misc{OpenCVFiltering,
   title = {OpenCV: Image Filtering},
   url = {https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html},
}
@inproceedings{lopez2009metodo,
   author = {López and Hans I},
   journal = {Ponencia Realizada en el Primer Congreso Regional de Electricidad, Electrónica y Sistemas, CREES},
   title = {Método alternativo para calcular la convolución de señales en tiempo continuo},
   year = {2009},
}
@book{Szeliski,
   author = {Richard Szeliski},
   doi = {10.1007/978-3-030-34372-9},
   isbn = {978-3-030-34371-2},
   publisher = {Springer International Publishing},
   title = {Computer Vision: Algorithms and Applications, 2nd ed.},
   url = {https://szeliski.org/Book/},
   year = {2022},
}
@article{fft,
   abstract = {Marzo 2013 Resumen: la Transformada Rápida de Fourier es un algoritmo que permite calcular eficientemente la Transforma de Fourier Discreta y su inversa. La Transformada Rápida de Fourier es de suma importancia en el análisis, diseño y realización de algoritmos y sistemas de procesamiento de señales dado que brinda mayor eficiencia tanto en tiempo como en recursos. Palabras clave: Transformada Rápida de Fourier, FFT, Transformada de Fourier Discreta, Procesamiento de Señales. I. INTRODUCCIÓN Toda señal periódica puede ser representada por la suma de series de Fourier. Con un análisis adecuado es posible obtener una representación de Fourier para señales de duración finita. Esta representación es la que se conoce como la Transformada de Fourier Discreta (TFD). La TFD se puede representar como: [ ] ∑ [ ] donde Se puede observar a simple vista que su resolución directa implica N multiplicaciones complejas y N-1 adiciones complejas por cada k. Por lo tanto, el cálculo directo de una TFD es de orden. Para valores pequeños de N la resolución en sí no consume mucho tiempo ni recursos. Sin embargo, para valores de N lo suficientemente grandes el cálculo directo se torna poco eficiente, no sólo por el gran tiempo que consume sino también por el acaparamiento de los recursos necesarios. Por ejemplo, para las operaciones a realizar serían ; asumiendo que cada operación toma aproximadamente 1ns el cálculo directo de la TFD tardaría unos 13343 días. Se puede ver, entonces, que el orden del cálculo directo impone un límite en aquellas aplicaciones que hacen uso de la TFD, especialmente las de tiempo real, dado que para valores mayores a cierto N el cálculo podrá resultar demasiado lento y los recursos disponibles podrán ser insuficientes. Es así que aparece la Transformada Rápida de Fourier (en inglés Fast Fourier Transform, FFT), un algoritmo para el cálculo eficiente de la TFD. Su importancia radica en el hecho que elimina una gran parte de los cálculos repetitivos a los que se ve sometida la TFD, por lo que se logra un cálculo más rápido a menor costo. El algoritmo de la FFT fue originalmente inventado por Carl Friedrich Gauss en 1805. Diferentes versiones del algoritmo fueron descubiertas a lo largo de los años, pero la FFT no se hizo popular sino hasta 1965, con la publicación de James Cooley y John Tukey, quienes reinventaron el algoritmo al describir como ejecutarlo de forma eficiente en una computadora. II. DESARROLLO DE LA FFT La idea básica detrás de la FFT consiste en la división del tiempo, es decir, en la descomposición iterativa en Transformadas de Fourier Discretas más simples. La FFT hace uso de dos propiedades de la Transformada de Fourier Discreta. La FFT presentada asume que N es potencia de 2, sin embargo, existen métodos para adaptar otros valores de N a las condiciones necesarias de este algoritmo. Las propiedades que se aprovechan son las siguientes:},
   author = {Ana Lucía},
   title = {FFT: Transformada Rápida de Fourier},
}
@misc{historiaIA,
   abstract = {Milenio Fundamentos de los Datos. Licenciado en Matemáticas por la Universidad de Chile y Ph.D. en Computer Science por la Wesleyan University. Líneas de investigación: fundamentos de los datos, bases de datos, lógica aplicada a la computa-ción y semántica de la Web. En 1842, la matemática y pionera de la informática, Ada Lovelace, programó el primer algoritmo destinado a ser pro-cesado por una máquina. Adelantada a su época, Ada especuló que la máquina "podría actuar sobre otras cosas ade-más de los números... el motor (la má-quina) podría componer piezas musica-les elaboradas y científicas de cualquier grado de complejidad o extensión". Dé-cadas más tarde, la visión de Ada es una realidad gracias a la Inteligencia Artificial (IA). Sin embargo, un hito con-siderado como el momento fundacional de la "inteligencia artificial", tanto del término como del campo de estudio, es una conferencia en Darmouth el año 1956 organizada por John McCarthy, Marvin Minsky, Claude Shannon y Na-thaniel Rochester [1]. En ella, los orga-nizadores invitaron a unos diez inves-tigadores para formalizar el concepto de inteligencia artificial como un nuevo campo de estudio científico. Pioneros de la IA, cuatro de los asistentes fueron posteriormente galardonados con el premio Turing (a menudo denominado Premio Nobel de informática) por sus contribuciones a la IA. Una idea común entre los asistentes, y profundamente arraigada hasta el día de hoy en el es-tudio de la IA, es que el pensamiento es una forma de computación no exclusiva de los seres humanos o seres biológi-cos. Más aún, existe la hipótesis de que la inteligencia humana es posible de re-plicar o simular en máquinas digitales. Ese mismo año dos de los participantes de la conferencia, Alan Newell y Herbert Simon, publican lo que es considerado el primer programa computacional de inteligencia artificial [2]. El programa "Logic Theory Machine" es capaz de descubrir demostraciones de teoremas en lógica simbólica. La idea principal es que a través de la combinación de simples operaciones primitivas, el pro-grama puede ir construyendo expresio-nes cada vez más complejas. El desafío computacional radica en encontrar la combinación de operaciones que de-muestran un teorema dado, entre una cantidad exponencial de posibles com-binaciones. La contribución de los auto-res fue idear un enfoque heurístico, o de reglas generales, que permiten recortar el árbol de búsqueda de manera "inte-ligente" y encontrar una solución en la mayoría de los casos, pero no siempre. La introducción de los procesos heurís-ticos han influenciado enormemente la ciencia de la computación y según los mismos autores, son la magia central en toda resolución de problemas hu-manos. No es coincidencia que esta tesis provenga de Herbert Simon, quien recibió el Nobel en economía por la pro-vocadora idea de modelar el compor-tamiento humano, no como un agente "homo economicus" totalmente racio-nal, sino que con "racionalidad limitada" cuya toma de decisiones es principal-mente heurística [3]. Dos paradigmas de investigación en IA IA simbólica La búsqueda heurística fue un pilar clave para los avances de la IA en sus comien-zos. Todo tipo de tareas de resolución de problemas, como probar teoremas y jugar ajedrez, implican tomar decisiones que se pueden modelar como un árbol de decisiones que debe ser recorrido para encontrar una estrategia que re-suelva el problema. Los algoritmos de búsqueda heurística son parte de una colección de métodos que se basan en representar el conocimiento implícito o procedimental que poseen los humanos de forma explícita, utilizando símbolos y reglas (legibles por humanos) en pro-gramas informáticos. La "IA simbólica" demostró ser muy exitosa en las prime-ras décadas de la IA logrando codificar},
   author = {Andrés Abeliuk and Claudio Gutiérrez},
   title = {Historia y
evolución de
la inteligencia
artificial},
}
@book{perceptrons,
   author = {Marvin Minsky and Seymour A. Papert},
   doi = {10.7551/MITPRESS/11301.001.0001},
   isbn = {9780262343930},
   journal = {Perceptrons},
   month = {1},
   publisher = {The MIT Press},
   title = {Perceptrons: An Introduction to Computational Geometry},
   year = {2017},
}
@book{nndesign,
   author = {Hagan Demuth and Beale De Jesús},
   title = {Neural Network Design 2nd Edition},
   url = {https://hagan.okstate.edu/NNDesign.pdf},
}
@article{funcionesActivacion,
   abstract = {Deep neural networks have gained remarkable achievements in many research areas, especially in computer vision, and natural language processing. The great successes of deep neural networks depend on several aspects in which the development of activation function is one of the most important elements. Being aware of this, a number of researches have concentrated on the performance improvements after the revision of a certain activation function in some specified neural networks. We have noticed that there are few papers to review thoroughly the activation functions employed by the neural networks. Therefore, considering the impact of improving the performance of neural networks with deep architectures, the status and the developments of commonly used activation functions will be investigated in this paper. More specifically, the definitions, the impacts on the neural networks, and the advantages and disadvantages of quite a few activation functions will be discussed in this paper. Furthermore, experimental results on the dataset MNIST are employed to compare the performance of different activation functions.},
   author = {Bin Ding and Huimin Qian and Jun Zhou},
   doi = {10.1109/CCDC.2018.8407425},
   isbn = {9781538612439},
   journal = {Proceedings of the 30th Chinese Control and Decision Conference, CCDC 2018},
   keywords = {activation function,deep architecture,neural network},
   month = {7},
   pages = {1836-1841},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Activation functions and their characteristics in deep neural networks},
   year = {2018},
}
@article{teoremaAproximacion,
   abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks. © 1989 Springer-Verlag New York Inc.},
   author = {G. Cybenko},
   doi = {10.1007/BF02551274/METRICS},
   issn = {09324194},
   issue = {4},
   journal = {Mathematics of Control, Signals, and Systems},
   keywords = {Approximation,Completeness,Neural networks},
   month = {12},
   pages = {303-314},
   publisher = {Springer-Verlag},
   title = {Approximation by superpositions of a sigmoidal function},
   volume = {2},
   url = {https://link.springer.com/article/10.1007/BF02551274},
   year = {1989},
}
@article{funcionesError,
   abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
   author = {Qi Wang and Yue Ma and Kun Zhao and Yingjie Tian},
   doi = {10.1007/S40745-020-00253-5/TABLES/8},
   issn = {21985812},
   issue = {2},
   journal = {Annals of Data Science},
   keywords = {Deep learning,Loss function,Machine learning,Survey},
   month = {4},
   pages = {187-212},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Comprehensive Survey of Loss Functions in Machine Learning},
   volume = {9},
   url = {https://link.springer.com/article/10.1007/s40745-020-00253-5},
   year = {2022},
}
