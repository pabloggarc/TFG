@article{Gao2010,
   abstract = {This paper proposes a method which combines Sobel edge detection operator and soft-threshold wavelet de-noising to do edge detection on images which include White Gaussian noises. In recent years, a lot of edge detection methods are proposed. The commonly used methods which combine mean de-noising and Sobel operator or median filtering and Sobel operator can not remove salt and pepper noise very well. In this paper, we firstly use soft-threshold wavelet to remove noise, then use Sobel edge detection operator to do edge detection on the image. This method is mainly used on the images which includes White Gaussian noises. Through the pictures obtained by the experiment, we can see very clearly that, compared to the traditional edge detection methods, the method proposed in this paper has a more obvious effect on edge detection. © 2010 IEEE.},
   author = {Wenshuo Gao and Lei Yang and Xiaoguang Zhang and Huizhong Liu},
   doi = {10.1109/ICCSIT.2010.5563693},
   isbn = {9781424455386},
   journal = {Proceedings - 2010 3rd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2010},
   keywords = {Edge detection,Edge detection operator,Soft-threshold wavelet de-noising,White Gaussian noises},
   pages = {67-71},
   title = {An improved Sobel edge detection},
   volume = {5},
   year = {2010},
}
@book{Goodfellow-et-al-2016,
   author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
   pages = {326-330},
   publisher = {MIT Press},
   title = {Deep Learning},
   url = {http://www.deeplearningbook.org},
   year = {2016},
}
@misc{OpenCVFiltering,
   title = {OpenCV: Image Filtering},
   url = {https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html},
}
@inproceedings{lopez2009metodo,
   author = {López and Hans I},
   journal = {Ponencia Realizada en el Primer Congreso Regional de Electricidad, Electrónica y Sistemas, CREES},
   title = {Método alternativo para calcular la convolución de señales en tiempo continuo},
   year = {2009},
}
@book{Szeliski,
   author = {Richard Szeliski},
   doi = {10.1007/978-3-030-34372-9},
   isbn = {978-3-030-34371-2},
   publisher = {Springer International Publishing},
   title = {Computer Vision: Algorithms and Applications, 2nd ed.},
   url = {https://szeliski.org/Book/},
   year = {2022},
}
@article{fft,
   abstract = {Marzo 2013 Resumen: la Transformada Rápida de Fourier es un algoritmo que permite calcular eficientemente la Transforma de Fourier Discreta y su inversa. La Transformada Rápida de Fourier es de suma importancia en el análisis, diseño y realización de algoritmos y sistemas de procesamiento de señales dado que brinda mayor eficiencia tanto en tiempo como en recursos. Palabras clave: Transformada Rápida de Fourier, FFT, Transformada de Fourier Discreta, Procesamiento de Señales. I. INTRODUCCIÓN Toda señal periódica puede ser representada por la suma de series de Fourier. Con un análisis adecuado es posible obtener una representación de Fourier para señales de duración finita. Esta representación es la que se conoce como la Transformada de Fourier Discreta (TFD). La TFD se puede representar como: [ ] ∑ [ ] donde Se puede observar a simple vista que su resolución directa implica N multiplicaciones complejas y N-1 adiciones complejas por cada k. Por lo tanto, el cálculo directo de una TFD es de orden. Para valores pequeños de N la resolución en sí no consume mucho tiempo ni recursos. Sin embargo, para valores de N lo suficientemente grandes el cálculo directo se torna poco eficiente, no sólo por el gran tiempo que consume sino también por el acaparamiento de los recursos necesarios. Por ejemplo, para las operaciones a realizar serían ; asumiendo que cada operación toma aproximadamente 1ns el cálculo directo de la TFD tardaría unos 13343 días. Se puede ver, entonces, que el orden del cálculo directo impone un límite en aquellas aplicaciones que hacen uso de la TFD, especialmente las de tiempo real, dado que para valores mayores a cierto N el cálculo podrá resultar demasiado lento y los recursos disponibles podrán ser insuficientes. Es así que aparece la Transformada Rápida de Fourier (en inglés Fast Fourier Transform, FFT), un algoritmo para el cálculo eficiente de la TFD. Su importancia radica en el hecho que elimina una gran parte de los cálculos repetitivos a los que se ve sometida la TFD, por lo que se logra un cálculo más rápido a menor costo. El algoritmo de la FFT fue originalmente inventado por Carl Friedrich Gauss en 1805. Diferentes versiones del algoritmo fueron descubiertas a lo largo de los años, pero la FFT no se hizo popular sino hasta 1965, con la publicación de James Cooley y John Tukey, quienes reinventaron el algoritmo al describir como ejecutarlo de forma eficiente en una computadora. II. DESARROLLO DE LA FFT La idea básica detrás de la FFT consiste en la división del tiempo, es decir, en la descomposición iterativa en Transformadas de Fourier Discretas más simples. La FFT hace uso de dos propiedades de la Transformada de Fourier Discreta. La FFT presentada asume que N es potencia de 2, sin embargo, existen métodos para adaptar otros valores de N a las condiciones necesarias de este algoritmo. Las propiedades que se aprovechan son las siguientes:},
   author = {Ana Lucía},
   title = {FFT: Transformada Rápida de Fourier},
}
@misc{historiaIA,
   abstract = {Milenio Fundamentos de los Datos. Licenciado en Matemáticas por la Universidad de Chile y Ph.D. en Computer Science por la Wesleyan University. Líneas de investigación: fundamentos de los datos, bases de datos, lógica aplicada a la computa-ción y semántica de la Web. En 1842, la matemática y pionera de la informática, Ada Lovelace, programó el primer algoritmo destinado a ser pro-cesado por una máquina. Adelantada a su época, Ada especuló que la máquina "podría actuar sobre otras cosas ade-más de los números... el motor (la má-quina) podría componer piezas musica-les elaboradas y científicas de cualquier grado de complejidad o extensión". Dé-cadas más tarde, la visión de Ada es una realidad gracias a la Inteligencia Artificial (IA). Sin embargo, un hito con-siderado como el momento fundacional de la "inteligencia artificial", tanto del término como del campo de estudio, es una conferencia en Darmouth el año 1956 organizada por John McCarthy, Marvin Minsky, Claude Shannon y Na-thaniel Rochester [1]. En ella, los orga-nizadores invitaron a unos diez inves-tigadores para formalizar el concepto de inteligencia artificial como un nuevo campo de estudio científico. Pioneros de la IA, cuatro de los asistentes fueron posteriormente galardonados con el premio Turing (a menudo denominado Premio Nobel de informática) por sus contribuciones a la IA. Una idea común entre los asistentes, y profundamente arraigada hasta el día de hoy en el es-tudio de la IA, es que el pensamiento es una forma de computación no exclusiva de los seres humanos o seres biológi-cos. Más aún, existe la hipótesis de que la inteligencia humana es posible de re-plicar o simular en máquinas digitales. Ese mismo año dos de los participantes de la conferencia, Alan Newell y Herbert Simon, publican lo que es considerado el primer programa computacional de inteligencia artificial [2]. El programa "Logic Theory Machine" es capaz de descubrir demostraciones de teoremas en lógica simbólica. La idea principal es que a través de la combinación de simples operaciones primitivas, el pro-grama puede ir construyendo expresio-nes cada vez más complejas. El desafío computacional radica en encontrar la combinación de operaciones que de-muestran un teorema dado, entre una cantidad exponencial de posibles com-binaciones. La contribución de los auto-res fue idear un enfoque heurístico, o de reglas generales, que permiten recortar el árbol de búsqueda de manera "inte-ligente" y encontrar una solución en la mayoría de los casos, pero no siempre. La introducción de los procesos heurís-ticos han influenciado enormemente la ciencia de la computación y según los mismos autores, son la magia central en toda resolución de problemas hu-manos. No es coincidencia que esta tesis provenga de Herbert Simon, quien recibió el Nobel en economía por la pro-vocadora idea de modelar el compor-tamiento humano, no como un agente "homo economicus" totalmente racio-nal, sino que con "racionalidad limitada" cuya toma de decisiones es principal-mente heurística [3]. Dos paradigmas de investigación en IA IA simbólica La búsqueda heurística fue un pilar clave para los avances de la IA en sus comien-zos. Todo tipo de tareas de resolución de problemas, como probar teoremas y jugar ajedrez, implican tomar decisiones que se pueden modelar como un árbol de decisiones que debe ser recorrido para encontrar una estrategia que re-suelva el problema. Los algoritmos de búsqueda heurística son parte de una colección de métodos que se basan en representar el conocimiento implícito o procedimental que poseen los humanos de forma explícita, utilizando símbolos y reglas (legibles por humanos) en pro-gramas informáticos. La "IA simbólica" demostró ser muy exitosa en las prime-ras décadas de la IA logrando codificar},
   author = {Andrés Abeliuk and Claudio Gutiérrez},
   title = {Historia y
evolución de
la inteligencia
artificial},
}
@book{perceptrons,
   author = {Marvin Minsky and Seymour A. Papert},
   doi = {10.7551/MITPRESS/11301.001.0001},
   isbn = {9780262343930},
   journal = {Perceptrons},
   month = {1},
   publisher = {The MIT Press},
   title = {Perceptrons: An Introduction to Computational Geometry},
   year = {2017},
}
@book{nndesign,
   author = {Hagan Demuth and Beale De Jesús},
   title = {Neural Network Design 2nd Edition},
   url = {https://hagan.okstate.edu/NNDesign.pdf},
}
@article{funcionesActivacion,
   abstract = {Deep neural networks have gained remarkable achievements in many research areas, especially in computer vision, and natural language processing. The great successes of deep neural networks depend on several aspects in which the development of activation function is one of the most important elements. Being aware of this, a number of researches have concentrated on the performance improvements after the revision of a certain activation function in some specified neural networks. We have noticed that there are few papers to review thoroughly the activation functions employed by the neural networks. Therefore, considering the impact of improving the performance of neural networks with deep architectures, the status and the developments of commonly used activation functions will be investigated in this paper. More specifically, the definitions, the impacts on the neural networks, and the advantages and disadvantages of quite a few activation functions will be discussed in this paper. Furthermore, experimental results on the dataset MNIST are employed to compare the performance of different activation functions.},
   author = {Bin Ding and Huimin Qian and Jun Zhou},
   doi = {10.1109/CCDC.2018.8407425},
   isbn = {9781538612439},
   journal = {Proceedings of the 30th Chinese Control and Decision Conference, CCDC 2018},
   keywords = {activation function,deep architecture,neural network},
   month = {7},
   pages = {1836-1841},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Activation functions and their characteristics in deep neural networks},
   year = {2018},
}
@article{teoremaAproximacion,
   abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks. © 1989 Springer-Verlag New York Inc.},
   author = {G. Cybenko},
   doi = {10.1007/BF02551274/METRICS},
   issn = {09324194},
   issue = {4},
   journal = {Mathematics of Control, Signals, and Systems},
   keywords = {Approximation,Completeness,Neural networks},
   month = {12},
   pages = {303-314},
   publisher = {Springer-Verlag},
   title = {Approximation by superpositions of a sigmoidal function},
   volume = {2},
   url = {https://link.springer.com/article/10.1007/BF02551274},
   year = {1989},
}
@article{funcionesError,
   abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
   author = {Qi Wang and Yue Ma and Kun Zhao and Yingjie Tian},
   doi = {10.1007/S40745-020-00253-5/TABLES/8},
   issn = {21985812},
   issue = {2},
   journal = {Annals of Data Science},
   keywords = {Deep learning,Loss function,Machine learning,Survey},
   month = {4},
   pages = {187-212},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {A Comprehensive Survey of Loss Functions in Machine Learning},
   volume = {9},
   url = {https://link.springer.com/article/10.1007/s40745-020-00253-5},
   year = {2022},
}
@article{descenso,
   abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
   author = {Sebastian Ruder},
   title = {An overview of gradient descent optimization algorithms},
}
@article{introCNN,
   author = {Jianxin Wu},
   title = {Introduction to Convolutional Neural Networks},
   year = {2017},
}
@misc{features,
   author = {Chris Kevin},
   title = {Feature Maps},
   url = {https://medium.com/@chriskevin_80184/feature-maps-ee8e11a71f9e},
}
@article{introCNN,
   abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks.},
   author = {Keiron O'shea and Ryan Nash},
   title = {An Introduction to Convolutional Neural Networks},
}
@article{backpropCNN,
   abstract = {This short note intends to present the analytical calculations involved in computing backpropagation for convolutional neural networks, in particular for the convolutional and pooling layers.},
   author = {Dimitrios Katsaros and Evangelia Fragkou and Dimitrios Papakostas},
   title = {Backpropagation for Convolutional Neural Networks},
}
@misc{softmax,
   author = {Thomas Kurbiel},
   title = {Derivative of the Softmax Function and the Categorical Cross-Entropy Loss},
   url = {https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1},
}
@article{transfer,
   abstract = {Deep learning has been the answer to many machine learning problems during the past two decades. However, it comes with two significant constraints: dependency on extensive labeled data and training costs. Transfer learning in deep learning, known as Deep Transfer Learning (DTL), attempts to reduce such reliance and costs by reusing obtained knowledge from a source data/task in training on a target data/task. Most applied DTL techniques are network/model-based approaches. These methods reduce the dependency of deep learning models on extensive training data and drastically decrease training costs. Moreover, the training cost reduction makes DTL viable on edge devices with limited resources. Like any new advancement, DTL methods have their own limitations, and a successful transfer depends on specific adjustments and strategies for different scenarios. This paper reviews the concept, definition, and taxonomy of deep transfer learning and well-known methods. It investigates the DTL approaches by reviewing applied DTL techniques in the past five years and a couple of experimental analyses of DTLs to discover the best practice for using DTL in different scenarios. Moreover, the limitations of DTLs (catastrophic forgetting dilemma and overly biased pre-trained models) are discussed, along with possible solutions and research trends.},
   author = {Mohammadreza Iman and Hamid Reza Arabnia and Khaled Rasheed},
   doi = {10.3390/TECHNOLOGIES11020040},
   issn = {2227-7080},
   issue = {2},
   journal = {Technologies 2023, Vol. 11, Page 40},
   keywords = {deep learning,deep transfer learning,machine learning,progressive learning,transfer learning},
   month = {3},
   pages = {40},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {A Review of Deep Transfer Learning and Recent Advancements},
   volume = {11},
   url = {https://www.mdpi.com/2227-7080/11/2/40/htm https://www.mdpi.com/2227-7080/11/2/40},
   year = {2023},
}
@article{overfitting,
   abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) "early-stopping" strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) "network-reduction" strategy is used to exclude the noises in training set; 3) "data-expansion" strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) "regularization" strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.},
   author = {Milan Decuyper and Mariele Stockhoff and Stefaan Vandenberghe and al - and Xue Ying},
   doi = {10.1088/1742-6596/1168/2/022022},
   issn = {1742-6596},
   issue = {2},
   journal = {Journal of Physics: Conference Series},
   month = {2},
   pages = {022022},
   publisher = {IOP Publishing},
   title = {An Overview of Overfitting and its Solutions},
   volume = {1168},
   url = {https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022 https://iopscience.iop.org/article/10.1088/1742-6596/1168/2/022022/meta},
   year = {2019},
}
@article{augm,
   abstract = {These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.},
   author = {Agnieszka Mikołajczyk and Michał Grochowski},
   doi = {10.1109/IIPHDW.2018.8388338},
   isbn = {9781538661437},
   journal = {2018 International Interdisciplinary PhD Workshop, IIPhDW 2018},
   keywords = {Machine learning,data augmentation,deep learning,medical imaging,style transfer},
   month = {6},
   pages = {117-122},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Data augmentation for improving deep learning in image classification problem},
   year = {2018},
}
@article{gan,
   abstract = {Despite the growing availability of high-quality public datasets, the lack of training samples is still one of the main challenges of deep-learning for skin lesion analysis. Gen-erative Adversarial Networks (GANs) appear as an enticing alternative to alleviate the issue, by synthesizing samples indistinguishable from real images, with a plethora of works employing them for medical applications. Nevertheless , carefully designed experiments for skin-lesion diagnosis with GAN-based data augmentation show favorable results only on out-of-distribution test sets. For GAN-based data anonymization-where the synthetic images replace the real ones-favorable results also only appear for out-of-distribution test sets. Because of the costs and risks associated with GAN usage, those results suggest caution in their adoption for medical applications.},
   author = {Alceu Bissoto and Eduardo Valle and Sandra Avila},
   pages = {1847-1856},
   title = {GAN-Based Data Augmentation and Anonymization for Skin-Lesion Analysis: A Critical Review},
   year = {2021},
}
@article{confusion,
   abstract = {Concise and unambiguous assessment of a machine learning algorithm is key to classifier design and performance improvement. In the multi-class classification task, where each instance can only be labeled as one class, the confusion matrix is a powerful tool for performance assessment by quantifying the classification overlap. However, in the multi-label classification task, where each instance can be labeled with more than one class, the confusion matrix is undefined. Performance assessment of the multi-label classifier is currently based on calculating performance averages, such as hamming loss, precision, recall, and F-score. While the current assessment techniques present a reasonable representation of each class and overall performance, their aggregate nature results in ambiguity when identifying false negative (FN) and false positive (FP) results. To address this gap, we define a method of creating the multi-label confusion matrix (MLCM) based on three proposed categories of multi-label problems. After establishing the shortcomings of current methods for identifying FN and FP, we demonstrate the usage of the MLCM with the classification of two publicly available multi-label data sets: i) a 12-lead ECG data set with nine classes, and ii) a movie poster data set with eighteen classes. A comparison of the MLCM results against statistics from the current techniques is presented to show the effectiveness in providing a concise and unambiguous understanding of a multi-label classifier behavior.},
   author = {Mohammadreza Heydarian and Thomas E. Doyle and Reza Samavi},
   doi = {10.1109/ACCESS.2022.3151048},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Classification performance,Confusion matrix,Machine learning,Multi-class,Multi-label},
   pages = {19083-19095},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {MLCM: Multi-Label Confusion Matrix},
   volume = {10},
   year = {2022},
}
@article{metricas_matriz,
   abstract = {We address the problems of 1/ assessing the confidence of the standard point estimates, precision, recall and F-score, and 2/ comparing the results, in terms of precision, recall and F-score, obtained using two different methods. To do so, we use a probabilistic setting which allows us to obtain posterior distributions on these performance indicators, rather than point estimates. This framework is applied to the case where different methods are run on different datasets from the same source, as well as the standard situation where competing results are obtained on the same data. © Springer-Verlag Berlin Heidelberg 2005.},
   author = {Cyril Goutte and Eric Gaussier},
   doi = {10.1007/978-3-540-31865-1_25/COVER},
   issn = {03029743},
   journal = {Lecture Notes in Computer Science},
   pages = {345-359},
   publisher = {Springer Verlag},
   title = {A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation},
   volume = {3408},
   url = {https://link.springer.com/chapter/10.1007/978-3-540-31865-1_25},
   year = {2005},
}
@article{auc,
   abstract = {Performance metrics in classification are fundamental in assessing the quality of learning methods and learned models. However, many different measures have been defined in the literature with the aim of making better choices in general or for a specific application area. Choices made by one metric are claimed to be different from choices made by other metrics. In this work, we analyse experimentally the behaviour of 18 different performance metrics in several scenarios, identifying clusters and relationships between measures. We also perform a sensitivity analysis for all of them in terms of several traits: class threshold choice, separability/ranking quality, calibration performance and sensitivity to changes in prior class distribution. From the definitions and experiments, we make a comprehensive analysis of the relationships between metrics, and a taxonomy and arrangement of them according to the previous traits. This can be useful for choosing the most adequate measure (or set of measures) for a specific application. Additionally, the study also highlights some niches in which new measures might be defined and also shows that some supposedly innovative measures make the same choices (or almost) as existing ones. Finally, this work can also be used as a reference for comparing experimental results in pattern recognition and machine learning literature, when using different measures.},
   author = {C Ferri and J Hernández-Orallo and R Modroiu},
   doi = {10.1016/j.patrec.2008.08.010},
   journal = {Pattern Recognition Letters},
   keywords = {Calibration,Classification,Performance measures,Ranking},
   pages = {27-38},
   title = {An experimental comparison of performance measures for classification},
   volume = {30},
   year = {2008},
}
@article{roc,
   abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. © 2005 Elsevier B.V. All rights reserved.},
   author = {Tom Fawcett},
   doi = {10.1016/J.PATREC.2005.10.010},
   issn = {0167-8655},
   issue = {8},
   journal = {Pattern Recognition Letters},
   keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
   month = {6},
   pages = {861-874},
   publisher = {North-Holland},
   title = {An introduction to ROC analysis},
   volume = {27},
   year = {2006},
}
@article{cuda_arq,
   abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA's parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
   author = {Jayshree Ghorpade and Jitendra Parande and Madhura Kulkarni and Amit Bawaskar},
   doi = {10.5121/acij.2012.3109},
   issue = {1},
   journal = {Advanced Computing: An International Journal ( ACIJ )},
   keywords = {ALU,CUDA,DirectCompute,GFLOPS,GPGPU,GPU,OpenCL,block,data parallelism,grid,thread},
   title = {GPGPU PROCESSING IN CUDA ARCHITECTURE},
   volume = {3},
   year = {2012},
}
@misc{cuda,
   author = {NVIDIA},
   title = {¿Qué es CUDA?},
   url = {https://support.nvidia.eu/hc/es/articles/4850516229266--Qu%C3%A9-es-CUDA},
}
@article{cuda_arq1,
   author = {Qian Kemao and Tianyi Wang},
   doi = {10.1117/3.2314949},
   journal = {GPU Acceleration for Optical Measurement},
   month = {12},
   publisher = {SPIE},
   title = {GPU Acceleration for Optical Measurement},
   year = {2017},
}
