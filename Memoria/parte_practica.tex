\chapter{Optimización del proceso de valoración de puntos de interés}

	Después de analizar diversos modelos de aprendizaje automático, sus características y algoritmos, se ofrece un ejemplo práctico que ilustra la aplicación de estos modelos para abordar un problema de la vida real y buscar una solución adecuada. Para ello se propone tratar de optimizar el proceso de valoración de puntos de interés de Wayfarer, tal y como se comentaba en la introducción. \\
	
	La valoración de un Wayspot (punto de interés) en Wayfarer consta de diferentes etapas. En primer lugar se muestra un título y descripción del Wayspot junto con la imagen que aparecería en los juegos. Además, aparece una imagen secundaria con una visión desde otra perspectiva, junto con otro texto que ayudarían al valorador a ubicar la propuesta, tal y como se muestra en la \Cref{fig:info_propuesta}. También aparece un breve cuestionario con una serie de preguntas genéricas que ayudan a determinar si aquello que se solicita cumple una serie de criterios (\Cref{fig:preguntas}). También se muestra un mapa que contiene los Wayspots cercanos para verificar que no exista ya, y comprobar con Street View la ubicación sugerida (\Cref{fig:mapa}). Finalmente, se pide clasificar la propuesta en una o varias categorías. 
	
	\begin{figure}[!h]
		\centering
		\includegraphics[scale = .4, valign = c]{info_principal}\hfill
		\includegraphics[scale = .4, valign = c]{imagen_principal}\hfill
		\includegraphics[scale = .4, valign = c]{imagen_secundaria}
		\caption{Información de una propuesta en Wayfarer}
		\label{fig:info_propuesta}
	\end{figure}
	
	\begin{figure}[!h]
		\centering
		\includegraphics[scale = .3]{cuestionario}
		\caption{Cuestionario de una propuesta en Wayfarer}
		\label{fig:preguntas}
	\end{figure}
	
	\begin{figure}[!h]
		\centering
		\includegraphics[scale = .3, valign = c]{mapa}\hfill
		\includegraphics[scale = .3, valign = c]{etiquetas}
		\caption{Mapa y clasificación de una propuesta de Wayfarer}
		\label{fig:mapa}
	\end{figure}
	
	\section{Tecnologías empleadas}
	
		\subsection{Python}
		
			Para realizar la codificación del proyecto se ha utilizado el lenguaje de programación Python. Es un lenguaje de alto nivel en el que se pueden seguir diferentes paradigmas de programación, como por ejemplo, el orientado a objetos o el funcional. Sus dos grandes ventajas son su sintaxis, que es muy sencilla y legible; y la gran variedad de bibliotecas que posee para realizar cualquier tipo de tareas, desde desarrollo web, proyectos de aprendizaje automático, desarrollo de videojuegos, creación de animaciones, etc. \\
			
			\begin{figure}[!h]
				\centering
				\includesvg{python}
				\caption{Python}
				\label{fig:py}
			\end{figure}
			
			Para este proyecto, las principales utilizadas son TensorFlow y Scikit-learn. La primera de ellas fue creada por Google y permite construir de manera sencilla modelos de aprendizaje automático, contando con las implementaciones de los principales elementos que los forman, así como algoritmos de entrenamiento, datasets de ejemplo, y funciones auxiliares de utilidad para trabajar con los datos empleados durante el proceso. La segunda de ellas, además de contar también con diversas implementaciones de modelos y algoritmos de aprendizaje automático, es bien conocida por contar con las implementaciones de diferentes métricas para modelos, lo que permite comprobar de manera rápida y visual la calidad de los modelos entrenados. Finalmente, estas bibliotecas hacen uso de otras como pueden ser Matplotlib, utilizada para generar cualquier tipo de figura desde Python. \\
			
			\begin{figure}[!h]
				\centering
				\begin{subfigure}{.5\textwidth}
					\centering
					\includesvg{tf}
					\caption{TensorFlow}
					\label{fig:tf}
				\end{subfigure}\hfill
				\begin{subfigure}{.5\textwidth}
					\centering
					\includesvg[scale = .7]{sklearn}
					\caption{Scikit-learn}
					\label{fig:sklearn}
				\end{subfigure}
				\caption{Bibliotecas populares de Python}
				\label{fig:bibliotecas}
			\end{figure}
			
			Por otro lado, no todo son ventajas. Python es un lenguaje interpretado y de tipado dinámico, lo que hace que su ejecución sea mucho más lenta que la de otros lenguajes. Sin embargo, existen soluciones como NumPy, una biblioteca que permite trabajar con grandes matrices de datos en tiempos de ejecución menores que al utilizar listas, gracias a que gran parte de su codificación se realizó utilizando lenguajes de más bajo nivel como C. 
		
		\subsection{CUDA}
		
			\gls{cuda} es una tecnología de la marca estadounidense NVIDIA. Esta tecnología hace referencia a un conjunto de elementos que permiten ejecutar código de manera masivamente paralela en una tarjeta gráfica de la compañía, de manera que el tiempo de ejecución es mucho menor que al ejecutarlo de manera secuencial. Entre estos elementos se encuentran el controlador, el compilador, las interfaces para los diferentes lenguajes (la más común la de C/C++), las librerías que contienen algoritmos optimizados, y los propios núcleos \gls{cuda} de la \gls{gpu} \cite{cuda}. \\
			
			\begin{figure}[!h]
				\centering
				\includesvg[width = .3\textwidth]{nvidia}
				\caption{NVIDIA}
				\label{fig:nvidia}
			\end{figure}
			
			El hardware de las tarjetas gráficas de NVIDIA se divide en \gls{sm}, que a su vez están compuestos de \gls{sp} o núclos \gls{cuda}, entre otros elementos. A nivel de software, la unidad básica paralela es el hilo, que a nivel de hardware se ejecuta en un \gls{sp}. Dichos \gls{sp}, son unos núcleos capaces de hacer operaciones más sencillas que las que un núcleo de una \gls{cpu} podría realizar, pero de manera más rápida. A su vez, los hilos se agrupan en bloques, teniendo en cuenta que a nivel de hardware cada bloque se ejecuta en un único \gls{sm}. Finalmente, todos los elementos se agrupan en una cuadrícula (o \textit{grid}) compuesta de bloques. Cada cuadrícula se ejecuta en una única \gls{gpu} a nivel de hardware, y ejecuta a nivel de software un kernel \cite{cuda_arq,cuda_arq1}. \\
			
			\begin{figure}[!h]
				\centering
				\includegraphics[scale = .3]{arquitectura_cuda}
				\caption{Arquitectura CUDA a nivel de hardware y software \cite{cuda_arq1}}
				\label{fig:arq_cuda}
			\end{figure}
			
			El hecho de poder realizar miles de operaciones sencillas de manera simultánea, reduce enormemente el tiempo de ejecución de algoritmos que son paralelizables, es decir, que contienen pasos sencillos que son independientes de otros y se pueden realizar al mismo tiempo, como es el caso de una convolución o multiplicación de matrices. Además, se cuenta con diferentes niveles de memoria, con los que se agilizan los tiempos de lectura de elementos que son utilizados frecuentemente. \\
			
			La mayoría de algoritmos de aprendizaje automático de TensorFlow tienen implementada su versión paralela, de manera que si se cuenta con una \gls{gpu} compatible con \gls{cuda}, TensorFlow la utilizará para ahorrar el mayor tiempo posible durante los entrenamientos de los modelos. En este caso se dispone de un equipo que cuenta con un procesador Intel Core i7-8700, una \gls{gpu} NVIDIA GeForce GTX 1060 6GB, y 32 GB de RAM; lo que hace posible la ejecución concurrente de dichos algoritmos. Esta tarjeta gráfica cuenta con 1280 \gls{sp} o núcleos \gls{cuda} repartidos en 10 \gls{sm}. \\
			
			Si bien el trabajo de indicar a TensorFlow que haga uso de \gls{cuda} es sencillo, no es tan trivial la configuración de drivers de NVIDIA, instalación de \gls{cuda}, versión de Python, versión de TensorFlow, versión de sistema operativo, etc. Con el frecuente cambio de versiones de cada componente, se presentan nuevas incompatibilidades entre ellos, haciendo que guías oficiales fallen y que el proceso de configuración inicial sea complicado en sistemas operativos como Windows 11. La manera más sencilla actualmente de realizar una instalación de \gls{cuda} y TensorFlow es sobre el sistema operativo Ubuntu en su versión 22.04 LTS. Para instalar y configurar el resto de componentes se ha hecho uso de LambdaStack (\url{https://lambdalabs.com/lambda-stack-deep-learning-software}). Mediante el uso del comando de terminal
			\begin{center}
				\begin{BVerbatim}[tabsize = 0]
					wget -nv -O- https://lambdalabs.com/install-lambda-stack.sh | sh -
				\end{BVerbatim}
			\end{center}
			automáticamente lanza LambdaStack que se encarga de instalar las versiones de Python y sus librerías (TensorFlow y PyTorch entre otras) adecuadas, los drivers de NVIDIA correctos, componentes de \gls{cuda}, realiza las configuraciones de variables de entorno, etc. En general, deja el entorno de Ubuntu configurado para poder trabajar en cualquier proyecto de inteligencia artificial en el que se quiera hacer uso de una \gls{gpu} de NVIDIA. También cuentan con una imagen de Docker que ofrece una experiencia similar. El uso de este servicio es completamente gratuito, y es el que utiliza la compañía que lo gestiona en sus servicios de cloud computing (que sí son de pago). \\
			
			Para probar la eficacia del uso de \gls{cuda} para proyectos de esta índole, se ha seleccionado un modelo de los que se presentan en la \Cref{subsec:crear_cnn} (entrenamiento de una red convolucional desde cero), y se ha lanzado midiendo el tiempo, el mismo entrenamiento pero con diferente número de épocas sobre \gls{cpu} y \gls{gpu}. La \Cref{fig:comparativa_cuda} evidencia cómo el entrenamiento en la \gls{gpu} llega a ser hasta cinco veces más rápido que en la \gls{cpu}. Todos los entrenamientos de la \Cref{subsec:crear_cnn} han sido realizados sobre la \gls{gpu}. 
			
			\begin{figure}[!h]
				\centering
				\includegraphics[scale = .5]{comp_cuda}
				\caption{Comparativa de entrenamiento en CPU frente a en GPU}
				\label{fig:comparativa_cuda}
			\end{figure}
			
		\subsection{Google Cloud Platform}
		
			\gls{gcp} es una de las plataformas de cloud computing más conocidas, junto con \gls{aws} y Microsoft Azure. Este tipo de plataformas se encargan de ofrecer servicios \gls{iaas}, \gls{paas}, y \gls{saas} muy útiles para empresas a la hora de desarrollar todo tipo de proyectos. Google divide los tipos de servicios que ofrece en diferentes categorías, siendo algunas de las principales las siguientes \cite{gcp}.  
			
			\begin{itemize}
				\item Inteligencia artificial y aprendizaje automático: \gls{gcp} cuenta con una amplia gama de servicios de inteligencia artificial en sus distintas áreas como por ejemplo, procesamiento de lenguaje, visión artificial, \gls{ia} generativa, etc. Su plataforma Vertex AI engloba una gran variedad de modelos muy útiles y permite entrenar, desplegar, y gestionar dichos modelos de manera sencilla. En general, se podrían categorizar como servicios \gls{paas} y \gls{saas}. 
				\item Inteligencia empresarial: Google ofrece herramientas (\gls{saas}) como podrían ser Looker y Looker Studio, que permiten trabajar con datos, analizarlos, visualizarlos, generar informes, y en general, todo tipo de tareas del campo de Business Intelligence. 
				\item Computación: dentro de este campo Google ofrece diferentes variantes de hardware virtualizado, como por ejemplo, máquinas virtuales, GPUs, ejecución de aplicaciones en contenedores, etc. En resumen, se trata de diferentes servicios \gls{iaas} y \gls{paas}
				\item Bases de datos y almacenamiento: Google ofrece múltiples servicios de almacenamiento de ficheros y bases de datos tanto \gls{sql} como NoSQL, de manera segura y escalable. De manera similar al apartado anterior, son principalmente servicios \gls{iaas} y \gls{paas}. 
			\end{itemize}
			
			La mayoría de estos servicios son accesibles de manera sencilla vía \gls{cli}, \gls{api}, o los diferentes \gls{sdk} de los lenguajes de programación más populares. 
			
			\begin{figure}[!h]
				\centering
				\includegraphics[scale = .2]{gcp}
				\caption{Google Cloud Platform}
				\label{fig:gcp}
			\end{figure}
			
			

	\section{Clasificación de imágenes mediante redes convolucionales}
	
		Una primera aproximación a agilizar el proceso de valoración comentado, sería detectar el objeto que se muestra en la imagen de la propuesta, para que en caso de que sea algo aceptable proseguir con el proceso de valoración, o en caso contrario, rechazar directamente la propuesta. En general, en función del objeto que aparezca en la imagen, se tienen directamente las respuestas a las preguntas de la \Cref{fig:preguntas}. \\
		
		Para comenzar este caso práctico, lo primero a realizar es el proceso conocido como \gls{etl}, que consiste en la extracción, transformación, y carga de los datos; para posteriormente poder trabajar con ellos y proporcionárselos al modelo. Comenzando por la extracción de datos, se presenta el primero de los problemas. La idea es utilizar imágenes que realmente hayan pasado por este proceso de valoración para poder hacer el proyecto lo más realista posible, sin embargo, ni Wayfarer ni ninguno de los juegos poseen alguna \gls{api} (al menos de manera pública) que permita recolectar de manera programática las imágenes utilizadas o información relativa a ellas. 
		
		\subsection{Proceso ETL}
		
			La solución adoptada para este proceso, ha sido recolectar manualmente imágenes que han superado el proceso de valoración, pudiendo consultar algunas de ellas desde el mapa de uno de los juegos (\url{https://intel.ingress.com/}). En este caso, se estarían clasificando entre las $n$ clases de objetos aceptables las imágenes recibidas, cosa que en un primer momento parece carecer de sentido pues se conoce el resultado de la valoración. Sin embargo, al no tener acceso a propuestas rechazadas, no se pueden recolectar estos datos para entrenar los modelos, pero si de la empresa responsable se tratase, se dispondría de una enorme cantidad de imágenes válidas y no válidas etiquetadas (gracias a la parte del proceso de valoración que muestra la \Cref{fig:mapa}), y que se podrían cargar de manera automática. En resumen, cambiando simplemente los datos que se cargarían y su fuente, se podrían tomar las siguientes decisiones sin necesidad de modificar el resto del proyecto.  
			
			\begin{itemize}
				\item Si $I \in C_i, 0 \leq i < m$, rechazar la propuesta
				\item Si $I \in C_j, m \leq j < n$, continuar evaluando la propuesta
			\end{itemize}
			
			Continuando con la extracción de los datos, y preparándolos para la carga, se ha creado una carpeta \texttt{tfg\_dataset} que representa el conjunto de las imágenes que se utilizan durante el proyecto. Dentro de esta, se ubicarán dos subcarpetas, \texttt{train} y \texttt{test}, que hacen referencia a las imágenes que se utilizarán para entrenar los modelos, y las que se utilizarán para evaluar su rendimiento. Para generar dichas carpetas partiendo de una que contiene las imágenes separadas por clases, se ha codificado la función \texttt{split\_tt} que permite hacer la división del conjunto en los de entrenamiento y test con el porcentaje especificado. Previamente a la ejecución de esta función, se recomienza ejecutar otra que se ha codificado llamada \texttt{renombrar\_imagenes}, que dada la ruta raíz donde se encuentran las carpetas de cada clase, renombra las imágenes de cada carpeta de manera que queden enumeradas. Esta es la manera de etiquetar el conjunto de datos y prepararlo para la carga. Se puede visualizar un breve esquema de esta estructura en la \Cref{fig:arbol_dataset}. \\
			
			\begin{figure}[!h]
				\centering
				\scriptsize
				\Tree[.\texttt{tfg\_dataset} [.\texttt{train} [.\texttt{clase\_1} $\vdots$ ] [.\texttt{clase\_2} \texttt{0.jpg} $\cdots$ \texttt{22.jpg} ] $\cdots$ [.\texttt{clase\_n} $\vdots$ ] ] [.\texttt{test} [.\texttt{clase\_1} $\vdots$ ] [.\texttt{clase\_2} \texttt{10.jpg} $\cdots$ \texttt{33.jpg} ]  $\cdots$ [.\texttt{clase\_n} $\vdots$ ] ] ]
				\caption{Árbol de carpetas del dataset}
				\label{fig:arbol_dataset}
			\end{figure}
			
			Para cargar el dataset en TensorFlow directamente desde la estructura de carpetas creadas, se hará uso de la función \texttt{image\_dataset\_from\_directory} de \texttt{utils}. Esta contiene una serie de parámetros interesantes a comentar. 
			
			\begin{itemize}
				\item \texttt{directory}: es el directorio raíz del dataset, en este caso \texttt{tfg\_dataset}. 
				\item \texttt{image\_size}: es una tupla de dos elementos con las dimensiones en píxeles que deberán tener las imágenes del dataset. 
				\item \texttt{labels}: mediante el valor \texttt{inferred} las etiquetas toman el mismo valor que el nombre de las carpetas. 
				\item \texttt{label\_mode}: hace referencia a la forma de codificar las etiquetas. Se empleará el valor \texttt{categorical} para codificar las etiquetas utilizando one-hot-encoding, es decir, si se tienen por ejemplo cuatro clases y un elemento pertenece a la cuarta, dicha etiqueta queda codificada como 0001. 
				\item \texttt{batch\_size}: hace referencia al tamaño de batch o lote que será utilizado durante el entrenamiento. Si en la función de entrenamiento se elige un tamaño diferente, se utilizará el menor de los valores. 
				\item \texttt{validation\_split}: hace referencia al porcentaje de los datos de entrenamiento que se reservan para validar el modelo durante el entrenamiento, es decir, permiten calcular el error del modelo al hacer una predicción de datos que nunca ha visto mientras entrena. Se reservarán un 20\% de los datos para validar. 
				\item \texttt{seed}: hace referencia a la semilla que se utiliza para ordenar las imágenes de manera aleatoria. 
				\item \texttt{subset}: seleccionando el valor \texttt{both} permite devolver el conjunto de entrenamiento y validación con la llamada a la función, es decir, \texttt{x\_train, x\_val = image\_dataset\_from\_directory(...)}. 
			\end{itemize}
			
			Como se puede observar, esta función que está haciendo principalmente el trabajo de la carga de datos, también hace parte del proceso de transformación, ya que permite modificar las dimensiones de la imagen (en este caso se utilizará un valor de $224 \times 224$ píxeles, se justificará más adelante), y también permite modificar la codificación de las etiquetas. Además, cada píxel (en cada canal de color) toma un valor entre 0 y 255. Es muy importante normalizar estos valores en el proceso de transformación para facilitar el trabajo a los diferentes modelos. Para ello, con ayuda de una capa de reescalado de Keras, se divide el valor de cada píxel entre 255 para obtener valores entre 0 y 1. Con motivo de verificar que todos estos pasos se han realizado correctamente, se codificado una función llamada \texttt{sample\_ds\_dfd} que permite visualizar nueve ejemplos aleatorios de un dataset creado con la función de TensorFlow mencionada, obteniendo como resultado la \Cref{fig:sample_dataset}. \\
			
			\begin{figure}
				\centering
				\includegraphics[scale = .8]{sample_dataset}
				\caption{Visualización de ejemplo de un dataset creado}
				\label{fig:sample_dataset}
			\end{figure}
			
		\subsection{Creación y entrenamiento de una red convolucional en TensorFlow}\label{subsec:crear_cnn}
		
			En esta primera aproximación se va a crear y entrenar una red convolucional desde cero con el dataset mencionado. En este caso, se han elegido cuatro tipos de objetos que frecuentemente se proponen como puntos de interés: parques infantiles, carteles informativos, marcadores de ruta, e hitos del Camino de Santiago. Mediante la clase \texttt{Sequential} de Keras, se puede proporcionar una lista de las capas que conforman el modelo. La primera de ellas será una capa de convolución, con la particularidad de que se debe indicar el tamaño de entrada, siendo este un tensor de las dimensiones indicadas en la creación del dataset. Se alterna cada capa de convolución \gls{relu} de stride $3 \times 3$ y 32 o 64 filtros, con una capa de maxpooling con stride $2 \times 2$. Estos valores han sido elegidos de manera arbitraria. Esta arquitectura se puede visualizar en la \Cref{fig:arq_cnn}. \\
			
			\begin{figure}[!h]
				\centering
				\includegraphics[scale = .55]{arq_cnn}
				\caption{Arquitectura de la red convolucional}
				\label{fig:arq_cnn}
			\end{figure}
			
			Mediante estas capas, se supone que la red debe extraer características de las imágenes, como por ejemplo
			
			\begin{itemize}
				\item Aparece un objeto rectangular
				\item Tiene dos patas 
				\item No tiene formas curvas
			\end{itemize}
			y de ahí con ayuda de una red clásica, ser capaz de deducir que en la imagen aparece un cartel. En realidad, las capas de convolución no obtienen características tan claras, pero sí resultan con una matriz de detalles en la imagen que pueden conducir a la misma conclusión. Para ello se utiliza la capa \texttt{Flatten} que transforma dicha matriz en un vector que sirve de entrada a la siguiente y última capa de la red, una capa oculta con tantas neuronas como clases diferentes. Como función de activación se utiliza softmax para obtener una distribución de probabilidad en la que se observe cuál es claramente la clase a la que pertenece el objeto, y con qué confianza lo es. Finalmente, mediante la función \texttt{summary} se puede observar un resumen del modelo. 
			
			\begin{verbatim}
				_________________________________________________________________
				Layer (type)                Output Shape              Param #   
				=================================================================
				conv2d (Conv2D)             (None, 222, 222, 32)      896       
				
				max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         
				D)                                                              
				
				conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     
				
				max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         
				g2D)                                                            
				
				conv2d_2 (Conv2D)           (None, 52, 52, 64)        36928     
				
				max_pooling2d_2 (MaxPoolin  (None, 26, 26, 64)        0         
				g2D)                                                            
				
				flatten (Flatten)           (None, 43264)             0         
				
				dense_2 (Dense)             (None, 8)                 346120    
				
				dense_3 (Dense)             (None, 4)                 36        
				
				=================================================================
				Total params: 402476 (1.54 MB)
				Trainable params: 402476 (1.54 MB)
				Non-trainable params: 0 (0.00 Byte)
				_________________________________________________________________
			\end{verbatim}
			
			Con la función \texttt{compile} se termina de crear el objeto que representa al modelo, pudiendo especificar un optimizador, la función de pérdida, y las métricas que se muestran durante el entrenamiento. Para este proyecto se utilizarán Adam, entropía cruzada, y precisión respectivamente. El modelo ya está en condiciones de ser entrenado, y esto se logra mediante la función \texttt{fit}, a la que se le proporcionan los datos de entrenamiento (\texttt{x\_train}), los datos de validación (\texttt{x\_val}), el número de épocas (en este caso se han elegido 25), y la ruta de los callbacks. Esta sirve para ir almacenando metadatos del entrenamiento, que mediante una utilidad con la que cuenta TensorFlow llamada TensorBoard, permite monitorizar de manera muy visual la calidad del entrenamiento, tal y como se muestra en la \Cref{fig:tb_cnn}. \\
			
			\begin{figure}[!h]
				\centering
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{evaluation_loss_vs_iterations_cnn}
					\caption{Iteraciones frente a pérdida}
					\label{fig:tb_cnn_a}
				\end{subfigure}\hfill
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{evaluation_accuracy_vs_iterations_cnn}
					\caption{Iteraciones frente a precisión}
					\label{fig:tb_cnn_b}
				\end{subfigure}
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{epoch_loss_cnn}
					\caption{Pérdida sobre el conjunto de entrenamiento y validación}
					\label{fig:tb_cnn_c}
				\end{subfigure}\hfill
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{epoch_accuracy_cnn}
					\caption{Precisión sobre el conjunto de entrenamiento y validación}
					\label{fig:tb_cnn_d}
				\end{subfigure}
				\caption{Pérdida y precisión durante el entrenamiento de la CNN}
				\label{fig:tb_cnn}
			\end{figure}
			
			La situación que describen las gráficas, es negativa y sería la típica no deseada, pues se observa como con el paso de las iteraciones el error crece mientras que la precisión se estanca en valores no deseados. En concreto, en la \Cref{fig:tb_cnn_c} se observa cómo el error sobre el conjunto de validación crece, mientras que sobre el conjunto de entrenamiento tiende a cero. Esta es una situación denominada como sobreaprendizaje u \textit{overfitting} \cite{overfitting}. La red no tiene capacidad de aprender ni generalizar, y lo que está haciendo es memorizar los datos de ejemplo que se le presentan, cometiendo entonces errores cuando recibe datos que no ha visto nunca. 
			
		\subsection{Transfer learning en TensorFlow}
		
			Como se ha podido observar durante la sección anterior, el resultado del entrenamiento no ha sido adecuado, pues la red tendía a memorizar los datos que se le presentaban sin mostrar una buena capacidad de generalización. Esto puede deberse a múltiples factores, como por ejemplo, que al tener un conjunto de datos de entrenamiento tan reducido no sea capaz de extraer correctamente las características que determinan a los objetos de cada clase. Es decir, en vez de hacerle llegar a las capas densas hechos como ``tiene un objeto rectangular'', ``tiene dos patas'', etc; podría estar haciéndole llegar, ``hay dos árboles en el fondo''. Observando la estructura del modelo, se está tratando de optimizar cerca un millón de parámetros con poco más de 500 observaciones, algo que es muy desproporcionado; la red no es capaz de encontrar relaciones con tan pocos datos. \\
			
			En proyectos similares del mundo real, y en los que además se dispone de pocos datos (como es el caso), es muy poco frecuente crear y entrenar un modelo desde cero tal y como se ha realizado en la \Cref{subsec:crear_cnn}. En vez de realizar esto, es muy común utilizar una técnica conocida como transfer learning. Tal y como se menciona en \cite{transfer}, es una técnica muy aplicada en casos en los que se dispone de pocos datos y los modelos de deep learning no son capaces de encontrar relaciones, y también en casos en los que se dispone de equipos con pocos recursos. Consiste en aplicar el conocimiento obtenido de otro dataset, no necesariamente relacionado, para facilitar el proceso de obtener el conocimiento deseado del dataset actual. \\
			
			En el problema que se está tratando, se va a aplicar esta técnica de la siguiente manera. Como previamente se ha mencionado, las redes convolucionales, pueden verse como un conjunto de capas capaces de extraer características de las imágenes, junto con una red neuronal clásica que es capaz de clasificar en función de dichas características. Teniendo en cuenta que el principal problema del modelo anterior podría ser que no fuera capaz de obtener las características adecuadas que definen a cada clase, la idea es utilizar una serie de capas que sí sean capaces de obtener de manera correcta las características de una imagen, para después poder entrenar la red neuronal clásica con las características adecuadas. \\
			
			En TensorFlow se pueden importar modelos ya entrenados. En este caso para aplicar la técnica de transfer learning, se ha decidido utilizar la red MobileNet de Google que se observa en la \Cref{fig:mb}. El modelo se puede importar como si de una capa se tratase al crear un modelo secuencial de Keras. Para lograr la técnica, será clave declarar que los parámetros de dicha capa no se deberán entrenar (pues al importar el modelo ya vienen con unos valores adecuados). A continuación se coloca una capa densa, encargada de clasificar las características que la red de Google extraiga, y que deberá ser entrenada. Esta vez, el número de parámetros a optimizar sí es más adecuado al número de observaciones disponibles. 
			
			\begin{figure}[!h]
				\centering
				\includegraphics[scale = .3]{mb}
				\caption{MobileNet \cite{mobilenet}}
				\label{fig:mb}
			\end{figure}
			
			\begin{verbatim}
				_________________________________________________________________
				Layer (type)                Output Shape              Param #   
				=================================================================
				mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   
				tional)                                                         
				
				global_average_pooling2d (  (None, 1280)              0         
				GlobalAveragePooling2D)                                         
				
				dense_2 (Dense)             (None, 8)                 10248     
				
				dense_3 (Dense)             (None, 4)                 36        
				
				=================================================================
				Total params: 2268268 (8.65 MB)
				Trainable params: 10284 (40.17 KB)
				Non-trainable params: 2257984 (8.61 MB)
				_________________________________________________________________
			\end{verbatim}
			
			La manera de lanzar el entrenamiento es la misma que en el anterior, sin embargo, esta vez los resultados obtenidos son totalmente distintos, tal y como muestran las gráficas de TensorBoard en la \Cref{fig:tb_tl}. \\
			
			\begin{figure}[!h]
				\centering
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{evaluation_loss_vs_iterations_mb}
					\caption{Iteraciones frente a pérdida}
					\label{fig:tb_tl_a}
				\end{subfigure}\hfill
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{evaluation_accuracy_vs_iterations_mb}
					\caption{Iteraciones frente a precisión}
					\label{fig:tb_tl_b}
				\end{subfigure}
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{epoch_loss_mb}
					\caption{Pérdida sobre el conjunto de entrenamiento y validación}
					\label{fig:tb_tl_c}
				\end{subfigure}\hfill
				\begin{subfigure}{.4\textwidth}
					\centering
					\includesvg[width = .85\textwidth]{epoch_accuracy_mb}
					\caption{Precisión sobre el conjunto de entrenamiento y validación}
					\label{fig:tb_tl_d}
				\end{subfigure}
				\caption{Pérdida y precisión durante el entrenamiento aplicando transfer learning}
				\label{fig:tb_tl}
			\end{figure}
			
			Estas gráficas representan una situación cercana a la ideal. En las \Cref{fig:tb_tl_a,fig:tb_tl_b} se observa cómo con el paso de las iteraciones, el error disminuye y la precisión aumenta hasta llegar a valores adecuados. Además, en las \Cref{fig:tb_tl_c,fig:tb_tl_d} se muestra claramente que tanto el error como la precisión se comportan de manera similar sobre los conjuntos de entrenamiento y validación respectivamente, tomando además valores adecuados. 
			
		\subsection{Aumento de datos}
		
			Como se ha visto a lo largo de las secciones anteriores, uno de los problemas que se está teniendo en este caso práctico, es la falta de datos. Una técnica utilizada frecuentemente en estos casos es conocida como aumento de datos. Consiste en modificar las imágenes del dataset de entrenamiento para que el modelo disponga de más ejemplos diferentes \cite{augm}. Podría entenderse como una parte del proceso de transformación \gls{etl}. \\
			
			Una vez más, es posible aplicar esta técnica mediante funciones de TensorFlow. Para ello se crea un objeto de la clase \texttt{ImageDataGenerator}. Mediante su constructor se pueden declarar los valores que toman los atributos que modificarán las imágenes, siendo algunos de los más destacables: 
			
			\begin{itemize}
				\item \texttt{rotation\_range}: gira la imagen. 
				\item \texttt{width\_shift\_range} y \texttt{height\_shift\_range}: desplazamiento horizontal y vertical de la imagen. 
				\item \texttt{shear\_range}: estira la imagen. 
				\item \texttt{zoom\_range}: hace zoom a la imagen. 
				\item \texttt{reescale}: indica el factor por el que se reescala la imagen, en este caso $1/255$ para facilitar el entrenamiento a la red, como en casos anteriores. 
				\item \texttt{validation\_split}: porcentaje de los datos reservados a validación. 
			\end{itemize}
			
			Una vez se tiene declarado el generador, se utiliza su método \texttt{flow\_from\_directory} para que dada la ruta que contiene las imágenes, el tamaño deseado de imagen, y el tamaño de los lotes, se cree el objeto que representa al dataset. De manera similar a la anterior, se ha programado una función \texttt{sample\_ds\_ffd} que permite visualizar nueve elementos del dataset creado, tal y como muestra la \Cref{fig:sample_dataset_au}. Esta es una de la manera más común de crear datos nuevos, siendo alguna de las más novedosas las arquitecturas \gls{gan}. Están formadas por dos redes, una capaz de crear imágenes y otra capaz de distinguir entre imágenes reales y generadas, y transmitir dicho conocimiento a la primera red, de manera que una intente ``engañar'' a la otra \cite{gan}. \\
			
			\begin{figure}
				\centering
				\includegraphics[scale = .8]{sample_dataset_au}
				\caption{Visualización de ejemplo de un dataset aplicando aumento de datos}
				\label{fig:sample_dataset_au}
			\end{figure}
			
			Contando ahora con el dataset original y el modificado, la técnica del aumento de datos se puede aplicar de dos formas. La primera de ellas es crear un modelo y entrenar únicamente con el datset modificado. Esto puede dar una mayor capacidad de generalización al modelo al mostrar imágenes más diferentes de objetos similares. La segunda de ellas consiste en entrenar el modelo con el dataset original, y después con el dataset modificado (sin cambiar qué imágenes pertenecen a los conjuntos de entrenamiento, validación, y test en cada dataset). Esta puede dar mejores resultados, pues tiene más observaciones y el modelo puede de ver el mismo objeto ``de diferentes formas''. Sin embargo hay que tener cuidado con esta segunda opción, pues podría conducir a situaciones de sobreajuste. 
			
		\subsection{Evaluación de los modelos}
		
			Si bien gracias a las gráficas que TensorBoard proporciona es posible hacerse una idea de la calidad que tendrán las predicciones de un modelo, no son suficientes. Será entonces el momento de presentar al modelo una serie de imágenes que nunca haya visto para poder evaluar la calidad de sus predicciones mediante una serie de métricas y valores estadísticos. Este conjunto de imágenes que el modelo no ha recibido hasta el momento, es el denominado conjunto de test. \\
			
			Para calcular estas métricas en Python, se va a utilizar la librería Scikit-learn, ya que contiene un conjunto de métodos con los que calcular y visualizar la mayoría de métricas empleadas en machine learning. Además, las predicciones sobre los conjuntos de test quedará representadas mediante dos matrices, \texttt{Y\_matrix} y \texttt{Y\_score}. La primera de ellas es una matriz $n \times 3$ que contiene para cada observación su etiqueta real, la predicha, y con qué probabilidad se le asigna. La segunda, contiene las probabilidades de cada observación de pertenecer a cada clase, por tanto es de dimensiones $n \times c$. 
			
			\begin{align*}
				Y_m &= \begin{pmatrix}
					0 & 0 & 0.75\\
					2 & 2 & 0.7\\
					3 & 1 & 0.97\\
					\vdots & \vdots & \vdots\\
					1 & 1 & 0.87
				\end{pmatrix} & 
				Y_s &= \begin{pmatrix}
					0.75 & 0.07 & 0.01 & 0.17\\
					0.15 & 0.01 & 0.7 & 0.14\\
					0.02 & 0.97 & 0.01 & 0.01\\
					\vdots & \vdots & \vdots & \vdots\\
					0.05 & 0.87 & 0.01 & 0.08
				\end{pmatrix}
			\end{align*}
			
			Dependiendo si el problema en cuestión es de regresión o clasificación, se deben utilizar unas métricas u otras. Debido a que se desea evaluar la calidad de una clasificación, se han elegido las siguientes. 
			
			\subsubsection{Matriz de confusión}\label{subsub:matriz_confusion}
			
				La matriz de confusión muestra en el caso de clasificación binaria, los verdaderos positivos, verdaderos negativos, falsos positivos, y falsos negativos, mientras que en el caso de clasificación no binaria, en general la relación entre las etiquetas reales de cada observación y las que el modelo le ha asignado \cite{confusion}. De manera muy visual se puede observar la cantidad de ejemplos de cada clase (pudiendo ver si se encuentran desbalanceadas), cuántos han sido clasificados correctamente, cuántos no, y en general entre qué clases suele confundirse más el modelo. \\
				
				\begin{figure}[!h]
					\centering
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .34]{mc_conv}
						\caption{Normal}
						\label{fig:mc_conv}
					\end{subfigure}\hfill
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .34]{mc_mb}
						\caption{Aplicando transfer learning}
						\label{fig:mc_mb}
					\end{subfigure}
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .34]{mc_convau}
						\caption{Aplicando aumento de datos}
						\label{fig:mc_convau}
					\end{subfigure}\hfill
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .34]{mc_mbau}
						\caption{Aplicando ambas}
						\label{fig:mc_mbau}
					\end{subfigure}
					\caption{Matrices de confusión}
					\label{fig:mc}
				\end{figure}
				
				En la \Cref{fig:mc} se observan las matrices de confusión de las cuatro situaciones descritas anteriormente. El elemento $a_{ij}$ representa la cantidad de observaciones de la clase $C_i$ que han sido clasificadas como $C_j$ (a partir de ahora, $\hat{C}_j$). La situación ideal se da cuando en el caso de que $i \neq j$, entonces $a_{ij} = 0$, o lo que es lo mismo, que solo haya valores  en la diagonal principal, pues a todos se les estaría asignando la clase correcta. Además, en esta representación gráfica sería deseable que todos los elementos de dicha diagonal tuviesen el mismo color, pues la situación actual describe un desbalanceo entre las clases. Se ve claramente que al aplicar transfer learning (\Cref{fig:mc_mb,fig:mc_mbau}) se obtienen resultados mucho mejores que al entrenar el modelo desde cero, tal y como las curvas de TensorBoard parecían indicar, pues la cantidad de malas clasificaciones es muy pequeña. La diferencia en dicho caso entre usar aumento de datos o no, no es muy significativa. \\
				
				Para poder calcular la matriz de confusión, se ha utilizado la función \texttt{confusion\_matrix} de Scikit-learn, que recibe como parámetros un vector con las clases reales de cada observación, y otro con las que el modelo les ha asignado. Mediante la función \texttt{ConfusionMatrixDisplay} se puede ver de manera gráfica la matriz (\Cref{fig:mc}). Con diferentes parámetros se pueden cambiar los colores e incrustar código \LaTeX{} para modificar los textos. Se ha codificado la función \texttt{matriz\_confusion} que recibe el dataset y $Y_m$, y muestra el gráfico con la configuración personalizada. 
				
			\subsubsection{Precisión, sensibilidad, y $F_1-$score}\label{subsub:f1}
				
				Si bien la matriz de confusión permite mostrar de manera visual la calidad de las clasificaciones, es conveniente calcular algunos valores \cite{metricas_matriz} a partir de esta matriz que permitan visualizar de manera numérica dichas calidades. La primera de ellas conocida como \textbf{precisión} $(\mathcal{P})$, y puede entenderse como la probabilidad de que un elemento que etiquetado como $C_i$, realmente pertenezca a dicha clase. 
				
				$$
				\mathcal{P} = P(C_i | \hat{C}_i) = \frac{P(C_i \cap \hat{C}_i)}{P(\hat{C}_i)}
				$$
				
				La siguiente se denomina \textbf{sensibilidad} (o recuerdo, $\mathcal{R}$) e indica la probabilidad de que un elemento de la clase $C_i$, se clasifique como tal. 
				
				$$
				\mathcal{R} = P(\hat{C}_i | C_i) = \frac{P(\hat{C}_i \cap C_i)}{P(C_i)}
				$$
				
				En resumen y en términos de probabilidad condicionada, $\mathcal{P}$ puede entenderse como una probabilidad a posteriori (cómo de bien ha clasificado el modelo), mientras que $\mathcal{R}$ puede entenderse como una probabilidad a priori (cómo de bien clasificaría el modelo). Es frecuente calcular la media armónica de estos dos valores para obtener un único valor que evalúe la calidad de clasificación del modelo, comúnmente llamada $F_1$. 
				
				$$
				F_1 = \frac{2}{\mathcal{P}^{-1} + \mathcal{R}^{-1}}
				$$
				
				Para calcular estos valores, se ha hecho uso de la función \texttt{classification\_report} de Scikit-learn, que muestra en una tabla estos valores para cada clase, junto con las medias de dichos valores. En situaciones de clases desbalanceadas como es esta, es conveniente mirar las medias ponderadas (\texttt{weighted avg}) para que malos resultados en clases con pocos elementos contribuyan de manera proporcional al resultado final. 
				
				\begin{align*}
					\overline{\mathcal{P}} &= \sum_{i=1}^n P(C_i)P(C_i | \hat{C}_i) &
					\overline{\mathcal{R}} &= \sum_{i=1}^n P(C_i)P(\hat{C}_i | C_i)
				\end{align*}
				
				\begin{figure}[!h]
					\centering
					\scriptsize
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_conv.txt}
						\caption{Normal}
						\label{fig:m_conv}
					\end{subfigure}\hfill
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_mb.txt}
						\caption{Aplicando transfer learning}
						\label{fig:m_mb}
					\end{subfigure}
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_convau.txt}
						\caption{Aplicando aumento de datos}
						\label{fig:m_convau}
					\end{subfigure}\hfill
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_mbau.txt}
						\caption{Aplicando ambas}
						\label{fig:m_mbau}
					\end{subfigure}
					\caption{Precisión, sensibilidad, y $F_1$}
					\label{fig:m}
				\end{figure}
				
				En las tablas obtenidas en la \Cref{fig:m}, puede verse como los valores obtenidos corresponden con las situaciones que mostraban las matrices de confusión, siendo los mejores en aquellas que se aplica transfer learning al tener valores mayores, y que a pesar de tener clases desbalanceadas (se puede observar en la columna de \texttt{support}), los valores entre clases son similares (en dichos casos), obteniendo medias ponderadas similares a las calculadas sin ponderar. En los casos más negativos, se observan valores más bajos y comportamientos diferentes dependiendo de la clase. 
			
			\subsubsection{Curvas ROC y AUC}
			
				Otra manera de evaluar la calidad de un clasificador binario es mediante las conocidas como curvas \gls{roc}, que representan en el espacio $[0, 1]\times[0, 1]$ los falsos positivos frente a los verdaderos positivos \cite{roc}. Es más sencillo de entender y representar en términos de probabilidad condicionada, representando $P(\hat{C}_i | \lnot C_i)$ frente a $P(\hat{C}_i | C_i)$. Para determinar entre dos curvas cuál describe un mejor clasificador, lo que se hace es elegir aquella con un \gls{auc} mayor. Si se toma la curva \gls{roc} como una función $r: [0, 1] \longrightarrow [0, 1]$, entonces el \gls{auc} se puede entender como 
				$$
				\mathcal{A} = \int_0^1 r(x)\,dx, 
				$$
				siendo $\mathcal{A} = 1$ el mejor de los valores posibles. En general, cuanto mayores sean los valores del eje $y$ para valores muy pequeños del eje $x$, mejor será el clasificador, mientras que cuando la curva \gls{roc} se aproxime a la función $f(x) = x$, más parecido será el modelo a hacer las clasificaciones al azar. En los casos que no son de clasificación binaria como este, no existe como tal una clase positiva y una negativa. Lo que se hace en su lugar es una de las siguientes aproximaciones \cite{auc}. 
				
				\begin{itemize}
					\item \gls{ovo}: para cada clase $C_i$ y $C_j$ con $i \neq j$, se toma una de ellas como clase positiva y la otra como negativa, y se calcula la curva \gls{roc} y \gls{auc} asociado, disponiendo de un total de $\binom{n}{2}$ o $n(n-1)$ curvas según el autor o la librería utilizada (depende de considerar o no cada elemento de la pareja como positiva y negativa, y viceversa, teniendo que $2\binom{n}{2} = n(n-1)$). 
					
					$$
					\mathcal{A}_{\text{OVO}} = \frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j \neq i}^n P(C_i)\mathcal{A}(C_i, C_j)
					$$
					
					\item \gls{ovr}: para cada clase $C_i$ se toma como clase positiva, y el resto de las clases como una única clase negativa. Se calcula la correspondiente curva \gls{roc} y \gls{auc} asociado, obteniendo un total de $n$ curvas. 
					
					$$
					\mathcal{A}_{\text{OVR}} = \sum_{i=1}^n P(C_i)\mathcal{A}(C_i, \lnot C_i)
					$$
					
				\end{itemize}
				
				Es muy útil visualizar en una misma gráfica las diferentes curvas \gls{roc} de una de estas técnicas, sin embargo Scikit-learn no cuenta con ninguna función que haga esto directamente, sólo muestra el valor final. Por ello, se ha codificado una función \texttt{roc\_auc\_ovr} que recibe $Y_m$, $Y_s$, y el nombre de las clases, y devuelve un gráfico en el que se ven las curvas \gls{roc} OVR de cada clase, los \gls{auc} de cada una, y el global. Para ello se ha hecho uso de la función \texttt{roc\_curve}, que calcula la curva, \texttt{auc}, que calcula el \gls{auc}, y \texttt{RocCurveDisplay} que muestra un gráfico de la curva \gls{roc}. En la \Cref{fig:roc} se muestran los cuatro resultados para cada uno de los casos, teniendo de nuevo que para las variantes de las \Cref{fig:roc_mb,fig:roc_mbau} se observan los mejores resultados sin haber casi diferencia entre ambos. \\
				
				\begin{figure}[!h]
					\centering
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .35]{auc_conv}
						\caption{Normal}
						\label{fig:roc_conv}
					\end{subfigure}\hfill
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .35]{auc_mb}
						\caption{Aplicando transfer learning}
						\label{fig:roc_mb}
					\end{subfigure}
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .35]{auc_convau}
						\caption{Aplicando aumento de datos}
						\label{fig:roc_convau}
					\end{subfigure}\hfill
					\begin{subfigure}{.4\textwidth}
						\centering
						\includegraphics[scale = .35]{auc_mbau}
						\caption{Aplicando ambas}
						\label{fig:roc_mbau}
					\end{subfigure}
					\caption{Curvas ROC}
					\label{fig:roc}
				\end{figure}
			
				Como resumen de esta parte, se puede concluir que el entrenamiento de una red convolucional desde cero es un proceso complejo y que requiere de una cantidad enorme de datos, que en los casos en los que no se dispone de ellos, conduce a resultados negativos, siendo necesario aplicar la técnica de transfer learning. A pesar de haber visto su gran eficacia, se va a realizar una prueba más. \\
				
				El dataset empleado, contenía imágenes de diferentes partes de España, por lo que el modelo estaba viendo variantes de cada tipo de objeto. Lo que se va a hacer ahora es separar dicho dataset de manera que contenga únicamente en el conjunto de test imágenes de la ciudad de Guadalajara. Al ser pocas, el experimento no será muy representativo, pero permitirá probar la eficacia de la red generalizando conceptos. Para ello se ha utilizado el mismo modelo que se ha utilizado en el caso de transfer learning (entrenando de nuevo los parámetros desde cero para evitar hacer predicciones de una imagen vista durante el entrenamiento). \\
				
				\begin{figure}[!h]
					\centering
					\includegraphics[scale = .65]{gu_vs_ga}
					\caption{Capacidad de generalización de la red, predicción en Galicia y Guadalajara}
					\label{fig:comparativa_gu}
				\end{figure}
				
				Como se observa en la \Cref{fig:comparativa_gu}, tras haber entrenado el modelo sin usar imágenes de objetos de Guadalajara, y enseñarle uno como por ejemplo este hito del Camino de Santiago, la red está casi segura que de uno de ellos se trata, a pesar de que sea completamente distinto a los que ha visto durante el entrenamiento con el aspecto típico de Galicia. 
				
			\subsection{Detección de clases desconocidas}
			
				Si bien hasta el momento se habían supuesto las condiciones ideales de que los modelos sólo recibían imágenes que contenían uno de los cuatro elementos seleccionados, en el problema que se trata de resolver aun habiendo entrenado un modelo para un número grande de clases, una persona podría enviar un punto de interés que no perteneciese a ninguna de las clases supuestas. En dicho caso, se necesitaría detectar que la imagen no debería pertenecer a ninguna de las clases que el modelo conoce, y descartar para ser evaluada de otra manera. \\
				
				Este problema es ampliamente conocido como \textit{open set recognition}, y existen diversas formas de abordarlo. En esta sección se va a demostrar la eficacia de dos de las más populares. En ambos casos se necesitan recopilar datos de imágenes que no pertenezcan a ninguna de las clases. Para esto, también se han tomado imágenes que superaron el proceso de valoración pero que difieren de las clases seleccionadas para este proyecto, como por ejemplo, grafitis, iglesias, fuentes, pistas deportivas, etc. \\
				
				La primera de las aproximaciones consiste en tratar el resto de clases desconocidas como una única clase desconocida. Al dataset utilizado en los casos anteriores se ha añadido una clase \texttt{resto} que contiene diversas imágenes mezcladas. Utilizando el mejor de los modelos anteriores (aplicando transfer learning), y teniendo en cuenta que no todas las imágenes de la misma clase guardan relación entre sí, se han obtenido resultados realmente positivos, puesto que el valor del $F_1$ es de $0.956$, casi no hay clasificaciones incorrectas en la matriz de confusión (\Cref{fig:mc_open_otros}), y las curvas \gls{roc} son casi perfectas (\Cref{fig:roc_open_otros}). \\
				
				\begin{figure}[!h]
					\centering
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_open_otros.txt}
						\caption{Precisión, sensibilidad, y $F_1$}
						\label{fig:m_open_otros}
					\end{subfigure}
					\begin{subfigure}{.5\textwidth}
						\centering
						\includegraphics[scale = .35]{mc_open_otros}
						\caption{Matriz de confusión}
						\label{fig:mc_open_otros}
					\end{subfigure}\hfill
					\begin{subfigure}{.5\textwidth}
						\centering
						\includegraphics[scale = .3]{auc_open_otros}
						\caption{Curvas ROC y AUC}
						\label{fig:roc_open_otros}
					\end{subfigure}
					\caption{Métricas en clasificación con clase desconocida}
					\label{fig:clase_otros}
				\end{figure}
				
				La segunda consiste en utilizar un primer modelo que permita distinguir entre imágenes de las clases conocidas y las que no. Para ello se utiliza un clasificador binario con las clases \texttt{resto} y \texttt{no\_resto}, y un modelo que sea capaz de distinguir entre las clases originales. Para realizar esto, ambos modelos siguen la arquitectura del mejor modelo de los anteriores, adaptando el número de clases. Se puede visualizar un resumen de esta situación en la \Cref{fig:openset}. \\
				
				\begin{figure}[!h]
					\includegraphics[width = \textwidth]{diagrama_open} 
					\caption{Arquitectura de clasificación con clases desconocidas}
					\label{fig:openset}
				\end{figure}
				
				En una primera aproximación podría parecer innecesario evaluar cada imagen en ambos modelos, pudiendo evaluar todas las imágenes en el clasificador binario, y aquellas que sean etiquetadas como \texttt{no\_resto}, ser evaluadas por el segundo clasificador. Sin embargo, esto es incorrecto, pues para medir la calidad de la arquitectura en general se necesita hacer uso de una matriz $Y_s$ (en este caso de dimensiones $n \times 5$) que contenga las probabilidades de pertenecer a cada clase, siendo dichas clases las $C_i$ y $R$. Si hay una imagen que se etiqueta como $R$, se tienen $P(R)$ y $P(\lnot R)$, pero no $P(C_i)$. Para obtener dichas probabilidades, se necesita hacer uso del segundo clasificador, teniendo en cuenta que las probabilidades que devuelve este son $P(C_i|\lnot R)$, tal y como se describe en la \Cref{fig:prob}. \\
				
				\begin{figure}[H]
					\centering
					% https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBoBGAXVJADcBDAGwFcYkQAdDnGADxwHAsAW3oBzGGAC+IKaXSZc+QigBMpYtTpNW7AEqz5IDNjwEi61VoYs2iTh0aQcAAgNyFp5UQAsFazp2IADCAPrkhp5K5ih+VjQ2uvZctFAQOAgexopmKsh+AMwBtuxhhFJaMFASCCigAGYAThDCSGQgOBBI5DQAFjD0UOyQYGwJgewACgAUegCUkSBNLW00nUjqIP2DwwRjIIz0AEYwjJM53vaNWGK9OCDjJfYzXE7pbgtZy62Im+uIBT6AyG9hG+0SQRmYXIAB9Xs4Potvhs1l1EABWIE7UF7B7aJ4gKGhMBwxwI+Z4w4nM4XGIga63e4VKRAA
					\begin{tikzcd}
						&  & R                                                                                      &  &        \\
						\texttt{imagen} \arrow[rru, "P(R)", no head] \arrow[rrd, "P(\lnot R)"', no head] &  &                                                                                        &  & C_1    \\
						&  & \lnot R \arrow[rru, "P(C_1|\lnot R)", no head] \arrow[rrd, "P(C_n|\lnot R)"', no head] &  & \vdots \\
						&  &                                                                                        &  & C_n   
					\end{tikzcd}
					\caption{Diagrama de probabilidades}
					\label{fig:prob}
				\end{figure}
				
				Una vez se dispone de $P(R)$, $P(\lnot R)$, y $P(C_i|\lnot R)$, basta aplicar el Teorema de la probabilidad total para calcular los $P(C_i)$, obteniendo que $P(C_i) = P(C_i | \lnot R) P(\lnot R)$. Para calcular entonces la matriz $Y_s$ necesaria, basta con tomar las matrices $Y_s^{(1)}$ y $Y_s^{(2)}$ (siendo cada una la obtenida por cada uno de los clasificadores), y corregir las probabilidades de la segunda matriz con la segunda columna de la primera matriz y concatenarla con la primera columna de la primera matriz, pudiéndose traducir esto a $Y_s = \left(\text{diag}\left(Y_{s_{i1}}^{(1)}\right)Y_s^{(2)}\middle|Y_{s_{i2}}^{(1)}\right)$, es decir, 
				$$
				\begin{gathered}
					Y_s^{(3)} = \begin{pmatrix}
						P_1(\lnot R) & 0 & \cdots & 0\\
						0 & P_2(\lnot R) & \cdots & 0\\
						\vdots & \vdots & \ddots & \vdots\\
						0 & 0 & \cdots & P_n(\lnot R)
					\end{pmatrix}
					\begin{pmatrix}
						P_1(C_1) & P_1(C_2) & \cdots & P_1(C_4)\\
						P_2(C_1) & P_2(C_2) & \cdots & P_2(C_4)\\
						\vdots & \vdots & \ddots & \vdots\\
						P_n(C_1) & P_n(C_2) & \cdots & P_n(C_4)
					\end{pmatrix}\\\\
					Y_s = \left(Y_s^{(3)} \middle| Y_{s_{i2}}^{(1)}\right). 
				\end{gathered}
				$$
				
				Por otro lado, ha de tenerse en cuenta que el primer clasificador da las etiquetas 0 y 1, y el segundo del 0 al 3, por lo que también se crea una matriz $Y_m$ en función de las matrices $Y_m^{(1)}$ y $Y_m^{(2)}$ para tener etiquetas del 0 al 4. Con ambas matrices construidas se pueden evaluar los resultados en la \Cref{fig:clase_otros_doble}, pudiendo observar que también son muy positivos e incluso similares a los obtenidos por la arquitectura anterior. Finalmente añadir, que es importante que ambos modelos se entrenen con las mismas imágenes de manera que el rendimiento entre ambos sea similar, pues si por ejemplo el primer clasificador es malo distinguiendo entre $R$ y $\lnot R$, entonces dará igual cómo de bueno sea el segundo clasificador separando entre las otras cuatro clases, y viceversa. Como conclusión, podría elegirse cualquiera de las dos, aunque a efectos prácticos la primera es más interesante al ser más simple y necesitar un único entrenamiento. 
				
				\begin{figure}[!h]
					\centering
					\begin{subfigure}{.5\textwidth}
						\centering
						\verbatiminput{img/mmc_open_bin_multiple.txt}
						\caption{Precisión, sensibilidad, y $F_1$}
						\label{fig:m_open_bin_multiple}
					\end{subfigure}
					\begin{subfigure}{.5\textwidth}
						\centering
						\includegraphics[scale = .35]{mc_open_bin_multiple}
						\caption{Matriz de confusión}
						\label{fig:mc_open_bin_multiple}
					\end{subfigure}\hfill
					\begin{subfigure}{.5\textwidth}
						\centering
						\includegraphics[scale = .35]{auc_open_bin_multiple}
						\caption{Curvas ROC y AUC}
						\label{fig:roc_open_bin_multiple}
					\end{subfigure}
					\caption{Métricas en clasificación con clase desconocida (doble modelo)}
					\label{fig:clase_otros_doble}
				\end{figure}
				
		\section{Clasificación de imágenes mediante transformers multimodales}
		
			El objetivo de esta sección será abordar de nuevo el problema resuelto anteriormente, ofreciendo una mejor solución al dejar abierta la posibilidad de trabajar con textos asociados a las imágenes, y no necesitar conocer el número de clases del dataset, tal y como se mostró en el marco teórico. En cuanto a los datos, se reutilizarán aquellos obtenidos del proceso \gls{etl} anterior, volcando todas las imágenes en un mismo directorio. \\
			
			De manera resumida, la solución está compuesta en primer lugar por un transformer multimodal, capaz de extraer conceptos o ideas representadas en diferentes modalidades, como textos, imágenes, o vídeos, y representarlos en un espacio común, para posteriormente hacer uso de un algoritmo de clasificación no supervisada que permita encontrar el número de clases óptimas, y clasificar los embeddings generados por el transformer. En la \Cref{fig:diagrama_embeddings} se muestra un diagrama de cómo deberían ser los embeddings que genera un modelo de este tipo. Si $\textbf{u}$ es el embedding de \texttt{rey}, $\textbf{v}$ el de \texttt{mujer}, y $\textbf{w}$ el de \texttt{reina}, entonces se espera que $\textbf{u} + \textbf{v} \approx \textbf{w}$. De la misma manera sucedería para una imagen o vídeo que represente los mismos conceptos. \\
			
			\begin{figure}[!h]
				\centering
				\includegraphics{diagrama_embeddings}
				\caption{Concepto de embeddings de un transformer multimodal}
				\label{fig:diagrama_embeddings}
			\end{figure}
			
			\subsection{Generación de embeddings con Vertex AI}
			
				Para abordar esta solución, la primera de las tareas es disponer del modelo capaz de extraer las ideas de las diferentes modalidades de información. Siendo una de las desventajas de los transformers el enorme volumen de datos de entrenamiento que necesitan para lograr un buen rendimiento, y los pocos datos que se poseen en este caso, no es razonable construir y entrenar un transformer desde cero. Por ello, se ha optado por utilizar un modelo previamente entrenado con grandes volúmenes de datos y que tiene un excelente rendimiento en tareas de esta índole. Dentro de los servicios que ofrece Google Cloud, su plataforma Vertex AI contiene una gran variedad de modelos, cada uno orientado a ciertos tipos de tareas. En concreto, se ofrece un modelo llamado \texttt{multimodalembedding} recomendado para tareas de clasificación y búsquedas semánticas de textos, imágenes, y vídeos \cite{vertex}. Dicho modelo sigue la arquitectura de un transformer multimodal codificador, es decir, recibe textos, imágenes, y vídeos, y es capaz de entregar un vector llamado embedding capaz de representar las ideas que aparecen en las entradas dadas. \\
				
				La manera de comunicarse con el modelo es vía \gls{api}. En la documentación se especifica que para el caso de las imágenes deben tener una resolución (máxima) de $512 \times 512$ píxeles y que deben enviarse codificadas en Base64 o el \gls{uri} de la imagen en caso de tenerla almacenada en un bucket de Google Cloud Storage. Para el caso de vídeo no se entrará en detalle pues no se utilizará en este proyecto, pero se puede generar un embedding por cada $x$ segundos de vídeo ignorando el contenido del audio. Además, debe estar almacenado en un bucket. En este caso se tienen almacenadas las imágenes en local, y para realizar esta tarea en Python se hace uso de la biblioteca \texttt{Pillow} para redimensionar las imágenes, \texttt{base64} para codificar la imagen redimensionada a Base64, y \texttt{requests} para realizar la llamada. Según indica la documentación, el cuerpo de la llamada \gls{http} POST, debe contener un fichero \gls{json} que sigue esta estructura,
				\begin{center}
					\begin{BVerbatim}[tabsize = 3]
						{
							"instances": [
								{
									"image": {
										"bytesBase64Encoded": "/9j/4AAQSkZJRgABAQAA...."
									}, 
									"text": "Un parque infantil con columpios...."
								}
							]
						}
					\end{BVerbatim}
				\end{center}
				para recibir como respuesta otro fichero \gls{json} que tiene el siguiente aspecto. 
				\begin{center}
					\begin{BVerbatim}[tabsize = 3]
						{
							"predictions": [
								{
									"textEmbedding": [
										0.010477379,
										[...]
										-0.0169572588,
										-0.00472954148
									],
									"imageEmbedding": [
										0.00262696808,
										[...]
										0.011650892,
										-0.00452344026
									]
								}
							]
						}
					\end{BVerbatim}
				\end{center}
				
				En el \gls{json} que se incluye en el cuerpo de la llamada, se puede elegir el número de dimensiones del embedding. Si no se indica nada, los embeddings tienen 1408 dimensiones. Además, en la cabecera de la llamada se debe indicar el token, que se obtiene en cada llamada mediante el \gls{cli} con ayuda del comando \texttt{gcloud auth print-access-token}, ya que por seguridad cambia periódicamente. Si la llamada ha sido exitosa, se recibe el código \texttt{HTTP 200} en la respuesta, y en caso contrario ha sucedido algún error. Además, se especifica que la cantidad máxima de llamadas por minuto y proyecto es de 120, que al realizar el siguiente cálculo se obtiene que no deben superarse las dos llamadas por segundo. 
				
				$$
				\left\lfloor\frac{120\text{ llamadas}}{60\text{ segundos}}\right\rfloor = 2 \text{ llamadas/segundo}
				$$
				
				El tiempo que transcurre desde que se realiza una llamada hasta que se obtiene la respuesta oscila entre $0.5$ y 1 segundos, por lo que el pausando el proceso $0.5$ segundos tras cada llamada se evita superar el límite fijado. Al superar el límite, se recibe el código \texttt{HTTP 429}. Finalmente, se almacenan todos los ficheros \gls{json} de las respuestas en disco, evitando así consumir  créditos y ahorrar tiempo en futuras ejecuciones con el mismo dataset. Una vez se han calculado el embedding correspondiente a cada imagen del dataset, es de utilidad crear un dataframe de Pandas que ayude a estructurar toda la información relativa a cada imagen obtenida hasta el momento para trabajar de manera más cómoda. En la \Cref{tab:df_ejemplo} se muestra un extracto de este dataframe. Durante la finalización del proyecto, se descubrió que por error humano se introdujeron algunas imágenes duplicadas en el dataset utilizado para trabajar con el transformer. Para eliminarlas del dataframe y evitar ``ensuciar'' los resultados, se introduce una columna que es el cálculo del hash SHA$-256$, y se eliminan las filas que tengan duplicada dicha columna. Una vez eliminados los elementos duplicados, se elimina la columna pues no será de más utilidad. 
				
				\begin{table}[!h]
					\scriptsize
					\centering
					\texttt{\input{img/embeddings1}}
					\caption{Ejemplo de dataframe}
					\label{tab:df_ejemplo}
				\end{table}
				
				Para reescalar las imágenes se ha codificado la función \texttt{reescalar\_img}; para generar los archivos \gls{json} con las imágenes en Base64, la función \texttt{img\_json}; y para realizar las peticiones al modelo, la función \texttt{get\_embedding}. 
				
			\subsection{Clasificación no supervisada de embeddings}
				
				Con motivo de ilustrar la salida del transformer, lo ideal es mostrar los embeddings en el espacio, pues si ha sido capaz de detectar diferencias y similitudes entre las imágenes, estas deberán verse reflejadas también en los embeddings. Visualizar vectores de 1408 componentes en el plano o espacio no es posible ``directamente'', pero gracias a algoritmos como \gls{pca} o \gls{tsne}, esto se hace posible. De manera intuitiva, estos algoritmos se comportan como una aplicación $\mathbb{R}^{1408} \longrightarrow \mathbb{R}^2$, intentando perder la mínima cantidad de información al realizar este cambio de dimensiones. No se entrará en detalle del funcionamiento de los algoritmos, pues no influyen en el resultado, simplemente permiten visualizar la situación. En la \Cref{fig:embeddingsR2} se observa el resultado obtenido por ambos algoritmos, distinguiendo claras nubes de puntos. 
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width = \textwidth]{reduced_embeddings}
					\caption{Embeddings en $\mathbb{R}^2$ mediante PCA y t-SNE}
					\label{fig:embeddingsR2}
				\end{figure}
				
				Es en este momento cuando se puede lanzar un algoritmo de clasificación no supervisada como $k-$medias o el de clustering jerárquico aglomerativo para clasificar los embeddings en $k$ clases sin necesidad de tener otro dataset de entrenamiento. En este caso, mediante el conocimiento del dataset se sabe que $k$ debe ser igual a 5. Por el contrario, si se dispone de millones de imágenes, no es viable revisar una a una para determinar el valor de $k$. Para resolver este problema, se puede utilizar el método del codo, explicado en el marco teórico. Tal y como se observa en la \Cref{fig:elbow}, se obtiene $k = 5$, de manera idéntica a lo que se conocía mediante la experiencia en el dataset. \\
				
				\begin{figure}[!h]
					\centering
					\includegraphics[scale = .5]{elbow_kmeans}
					\caption{Cálculo del $k$ óptimo mediante el método del codo}
					\label{fig:elbow}
				\end{figure} 
				
				Tras utilizar este valor de $k$ calculado, se pueden lanzar los algoritmos $k-$means y \gls{cja} con este valor, y colorear los embeddings de la \Cref{fig:embeddingsR2} para visualizar los clusters detectados por ambos algoritmos. En las \Cref{fig:clusters_kmeans,fig:clusters_cja} se observa cómo aparentemente ambos algoritmos han sido capaces de separar las nubes de puntos. Podría parecer que algunas imágenes están más cerca del centro de un cluster que del que tienen asignado, pero es importante recordar que el proceso de clustering se ha realizado en un espacio de 1408 dimensiones, mientras que estas figuras describen una situación similar en $\mathbb{R}^2$. En el hipotético caso de poder visualizar $\mathbb{R}^{1408}$, sería más evidente la asignación. 
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width = \textwidth]{reduced_embeddings_k-means}
					\caption{Clusters calculados con $k-$means}
					\label{fig:clusters_kmeans}
				\end{figure} 
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width = \textwidth]{reduced_embeddings_Clustering jerárquico aglomerativo}
					\caption{Clusters calculados con CJA}
					\label{fig:clusters_cja}
				\end{figure} 
				
				Con motivo de poder ilustrar de manera conjunta el resultado del cálculo de embeddings y la clasificación de estos, se muestran en la \Cref{fig:embeddings_visuales} los embeddings representados con ayuda de \gls{tsne} y los clusters calculados por $k-$means, con la diferencia que esta vez los embeddings se sustituyen por sus imágenes correspondientes, y aparecen enmarcadas con el color de su cluster. Con motivo de facilitar la visualización de los clusters, se ha dibujado un círculo de radio el percentil 90 de las distancias al centroide (para evitar que outliers ensucien el dibujo), y centrado en este. \\
				
				\begin{figure}[!h]
					\centering
					\includegraphics[width = \textwidth]{images_clusters}
					\caption{Imágenes y clusters}
					\label{fig:embeddings_visuales}
				\end{figure} 
				
				En esta situación es mucho más fácil visualizar el resultado obtenido. Se puede observar cómo el transformer ha colocado próximas entre sí las imágenes de cada una de las cuatro clases originales. También ha sido capaz de colocar próximas entre sí a las imágenes que pertenecen a la clase resto, viendo cómo aparecen juntos cada uno de los tipos de objetos que conforman dicha clase, murales, piscinas y fuentes, monumentos, recintos deportivos, esculturas religiosas, etc. Además, hay casos en los que ha ubicado alguna imagen entre varios clusters bien diferenciados, como es el caso de un cartel que aparece entre el cluster de los carteles, el de los parques, y el de otros, en concreto cerca de los elementos deportivos. Pero esto no resulta ser desafortunado, pues al hacer zoom sobre dicha imagen se observa un cartel indicando las normas de un parque con máquinas deportivas. Finalmente, se observa que mediante los colores de los bordes, el algoritmo de clasificación no supervisada ha sido capaz de juntar en el mismo cluster imágenes similares. 
				
			\subsection{Evaluación de la clasificación}
			
				Una vez calculadas las clases o clusters de cada imagen con los dos algoritmos propuestos, se añaden como columnas al dataframe con motivo de agilizar el trabajo de evaluar la calidad de las clasificaciones de embeddings. La \Cref{tab:df_clusters} muestra un extracto. A la hora de realizar la evaluación de la clasificación de los embeddings pueden darse dos situaciones, conocer o no las clases reales. En este caso aprovechando que se conocen las reales, se han abordado ambas situaciones.
			
				\begin{table}[!h]
					\tiny
					\centering
					\texttt{\input{img/embeddings2}}
					\caption{Dataframe con clusters calculados}
					\label{tab:df_clusters}
				\end{table}
				
				\subsubsection{Índice rand ajustado}
				
					En el caso en el que se conocen las clases reales, una de las métricas más populares es el \gls{ari} \cite{ari}. Se supone que se dispone de dos particiones del dataset original, $\mathcal{C} = \{C_1, C_2, \ldots, C_r\}$ y $\mathcal{D} = \{D_1, D_2, \ldots, D_s\}$, donde una de ellas es la generada por el algoritmo, y la otra es la original. Con ayuda de cada uno de estos conjuntos se construye la llamada matriz de contingencia, donde la entrada $n_{ij}$ representa el valor de $|C_i \cap D_j|$. En la \Cref{fig:matrices_contingencia} se observan las matrices de contingencia para los clusters generados por los dos algoritmos elegidos. \\
					
					\begin{figure}[!h]
						\centering
						\includegraphics[width = \textwidth]{matrices_contingencia}
						\caption{Matrices de contingencia}
						\label{fig:matrices_contingencia}
					\end{figure} 
					
					Estas matrices ayudan a dar una idea de la correspondencia entre los clusters originales y los calculados, observando los elementos que destacan por filas o columnas. Son el primer paso para calcular el \gls{ari}, pues sería erróneo pensar que se trata de una clasificación incorrecta el asignar una imagen al cluster $i$, cuando originalmente tenía asignado el cluster $j$, pues puede que sean clusters similares o incluso el mismo, pero que el algoritmo los haya nombrado de diferente manera. Para solucionar este problema, lo que se hace es trabajar con todas las posibles parejas del dataset, y comprobar si pertenecen a las mismas parejas de clusters en ambas particiones, dando lugar a los siguientes cuatro casos. 
					
					\begin{itemize}
						\item $(u \in C_p \,\land\, v \in C_p) \,\land\, (u \in D_q \,\land\, v \in D_q)$
						\item  $(u \in C_p \,\land\, v \in C_p) \,\land\, (u \in D_q \,\land\, v \not\in D_q)$
						\item  $(u \in C_p \,\land\, v \not\in C_p) \,\land\, (u \in D_q \,\land\, v \in D_q)$
						\item  $(u \in C_p \,\land\, v \not\in C_p) \,\land\, (u \in D_q \,\land\, v \not\in D_q)$
					\end{itemize}
					
					A partir de estos casos obtenidos de las matrices de contingencia, se construyen las matrices de confusión que se observan en la \Cref{fig:matrices_confusion_transf}, y que representan la misma idea que en la \Cref{subsub:matriz_confusion}. Se desea que el valor en los elementos de la diagonal principal sea máximo, pues representa el número $A$ de acuerdos; y que sean mínimos el resto, pues representan el valor de desacuerdos $D$. \\
					
					\begin{figure}[!h]
						\centering
						\includegraphics[width = \textwidth]{matrices_confusion_transf}
						\caption{Matrices de confusión}
						\label{fig:matrices_confusion_transf}
					\end{figure} 
					
					En este punto se puede calcular el índice rand $\mathcal{R}_I$, que sería el equivalente a la precisión $\mathcal{P}$ presentada en la \Cref{subsub:f1}, pues se calcula como la tasa de acuerdos. En la siguiente ecuación se muestra su cálculo en base a la matriz de contingencia, donde $n_{i\cdot}$ representa la suma de las columnas de la fila $i$, y $n_{\cdot j}$ la suma de las filas de la columna $j$. 
					
					$$
					\mathcal{R}_I = \frac{A}{A + D} = \frac{\displaystyle\binom{n}{2} + 2\sum_{i = 1}^r\sum_{j = 1}^s \binom{n_{ij}}{2} - \sum_{i = 1}^r \binom{n_{i\cdot}}{2} - \sum_{j = 1}^s \binom{n_{\cdot j}}{2}}{\displaystyle\binom{n}{2}}
					$$
					
					Una vez conocido el valor del índice rand, es inmedianto calcular su versión ajustada. Su misión es evitar un valor adecuado por particiones al azar, pues un valor alto de $\mathcal{R}_I$ no necesariamente implica una buena calidad de clustering. Se podría por ejemplo, de manera aleatoria separar el dataset en dos clusters dos veces. Si el número de elementos del dataset es elevado, no sería raro observar que bastantes elementos aparecen en los mismos clusters en ambas particiones, lo que según el $\mathcal{R}_I$ indicaría un buen clustering cuando probablemente no sea así. Se calcula de la siguiente manera. 
					
					$$
					\mathcal{R}_{A_I} = \frac{\mathcal{R}_{I} - E[\mathcal{R}_I]}{\max\{\mathcal{R}_I\} - E[\mathcal{R}_I]} = \frac{\displaystyle\sum_{i = 1}^r\sum_{j = 1}^s\binom{n_{ij}}{2} - \binom{n}{2}^{-1}\sum_{i = 1}^r\binom{n_{i\cdot}}{2}\sum_{j = 1}^s\binom{n_{\cdot j}}{2}}{\displaystyle\frac{1}{2}\left(\sum_{i = 1}^r \binom{n_{i\cdot}}{2} + \sum_{j = 1}^s \binom{n_{\cdot j}}{2}\right) - \binom{n}{2}^{-1}\sum_{i = 1}^r\binom{n_{i\cdot}}{2}\sum_{j = 1}^s\binom{n_{\cdot j}}{2}}
					$$
					
					Valores próximos a 1 indican un buen clustering, cercanos a 0 similar a hacerlo de manera aleatoria, y cercanos a $-1$ (en general menor que cero) peor que de manera aleatoria. Para la clasificación realizada con $k-$means se obtiene un \gls{ari} de $0.944$, y con \gls{cja} $0.953$. Ambos resultados son muy adecuados. 
					
				\subsubsection{Homogeneidad, completitud, y $V-$measure}
				
					De manera similar que la $F_1-$score relacionaba la precisión y la sensibilidad en el contexto de modelos de clasificación, en el caso de algoritmos de clustering en los que se conocen las clases reales es popular utilizar la $V-$measure, que indica cómo de bueno es el clustering realizado en función de la homogeneidad y completitud de los clusters. Dichos cálculos se basan en la entropía, entropía condicionada y entropía conjunta. 
					
					$$
					\begin{gathered}
						H(\mathscr{C}) = \sum_{i = 1}^{|\mathcal{C}|}P(\mathscr{C}_i)\log_2(P(\mathscr{C}_i))\\
						\begin{aligned}
							H(\mathscr{C} | \mathscr{D}) &= -\sum_{i = 1}^{|\mathcal{C}|}\sum_{j = 1}^{|\mathcal{D}|} P(\mathscr{C}_i \cap \mathscr{D}_j) \log_2 (P(\mathscr{C}_i | \mathscr{D}_j)) &
							H(\mathscr{C}, \mathscr{D}) &= -\sum_{i = 1}^{|\mathcal{C}|}\sum_{j = 1}^{|\mathcal{D}|} P(\mathscr{C}_i \cap \mathscr{D}_j) \log_2 (P(\mathscr{C}_i \cap \mathscr{D}_j))
						\end{aligned}
					\end{gathered}
					$$
					
					En las ecuaciones anteriores \cite{entropia}, los términos $\mathscr{C}$ y $\mathscr{D}$ son variables aleatorias discretas que representan los clusters de las particiones $\mathcal{C}$ y $\mathcal{D}$ a los que pertenece un determiando elemento, y abreviando, $P(\mathscr{C}_i) = P(\mathscr{C} = C_i)$. El concepto de homogeneidad $h$ hace referencia a que cada cluster contenga únicamente elementos de una misma clase; mientras que el de completitud $c$, que todos los miembros de una clase se asignen al mismo cluster \cite{vmeasure}. 
					
					$$
					\begin{aligned}
						h &= \begin{cases}
							1 & \text{si}\, H(\mathscr{C}, \mathscr{D}) = 0\\
							1 - \frac{H(\mathscr{C}|\mathscr{D})}{H(\mathscr{C})} & \text{si}\, H(\mathscr{C}, \mathscr{D}) \neq 0\\
						\end{cases} & 
						c &= \begin{cases}
							1 & \text{si}\, H(\mathscr{D}, \mathscr{C}) = 0\\
							1 - \frac{H(\mathscr{D}|\mathscr{C})}{H(\mathscr{D})} & \text{si}\, H(\mathscr{D}, \mathscr{C}) \neq 0\\
						\end{cases}
					\end{aligned}
					$$
					
					Para los algoritmos de clustering presentados, se obtienen unos valores $(h, c, V)$ adecuados. En el caso de $k-$means se obtiene $(0.929, 0.934, 0.932)$, y para \gls{cja} $(0.937, 0.938, 0.938)$. La $V-$measure se calcula de la siguiente manera. 
					
					$$
					V = \frac{2hc}{h + c}
					$$
					
					No es recomendable utilizar esta métrica de manera exclusiva en casos con pocas observaciones (menos de 1000) o un número elevado de clusters (más de 10). En dichos casos es recomendable respaldarla con otras como el \gls{ari}. En este caso, se ve cómo ambas indican un buen clustering.
					
				\subsubsection{Coeficiente de la silueta}
				
					 Otra métrica popular para medir la calidad de un clustering es el coeficiente de la silueta \cite{formulas_silhouette}. Esta no necesita conocer las clases reales, pues mide cómo de parecido es un elemento $\textbf{x}_i$ a los de su cluster, es decir, cómo de compactos son los clusters; y además mide la diferenciación entre clusters, es decir, cómo de separados están. 
					 
					 $$
					 \begin{gathered}
					 	a_i = \frac{1}{|C_l| - 1}\sum_{\substack{\textbf{x}_j\in C_l\\ i\neq j}}\|\textbf{x}_i - \textbf{x}_j\|\\
					 	b_i = \min_{m \neq l}\left\lbrace\frac{1}{|C_m|}\sum_{\textbf{x}_j\in C_m}\|\textbf{x}_i - \textbf{x}_j\|\right\rbrace\\
					 	\mathcal{S} = \frac{1}{n}\sum_{i = 1}^{n}\frac{b_i - a_i}{\text{máx}\{a_i, b_i\}}
					 \end{gathered}
					 $$
					 
					 El cómo de compactos son los clusters lo miden los $a_i$, mientras que la separación entre clusters lo miden los $b_i$. El valor de $\mathcal{S}$ varía entre $-1$ y 1, indicando los valores cercanos a 1 un clustering perfecto (puntos parecidos entre sí en un mismo cluster, y clusters separados), valores cercanos a $-1$ un mal clustering (parecería más razonable asignar algunos puntos a otros clusters), y valores cercanos a cero indican que no estaría claro a qué cluster deberían asignarse algunos puntos al no estar bien separados los clusters \cite{interpretacion_silhouette}. \\
					 
					 \begin{figure}[!h]
					 	\centering
					 	\begin{subfigure}{.45\textwidth}
					 		\centering
					 		\includegraphics[width = \textwidth]{silhouette_kmeans}
					 		\caption{Coeficiente de la silueta para $k-$means}
					 		\label{fig:silhouette_kmeans}
					 	\end{subfigure} 
					 	\begin{subfigure}{.45\textwidth}
					 		\centering
					 		\includegraphics[width = \textwidth]{elbow_sil_kmeans}
					 		\caption{Método del codo con el coeficiente de la silueta}
					 		\label{fig:silhouette_elbow}
					 	\end{subfigure} 
					 	\caption{Coeficiente de la silueta}
					 	\label{fig:silhouette}
					 \end{figure}
					 
					 En la \Cref{fig:silhouette_kmeans} se muestra el cálculo del coeficiente de la silueta para el clustering realizado con $k-$means. Se obtiene un valor próximo a $0.1$, indicando que el clustering no es bueno en cuanto a separación de los clusters. De aquí no se deduce que el clustering sea malo, simplemente se deduce que los clusters no están lo suficientemente separados para que esta métrica lo considere un buen clustering. Al aplicar el método del codo con el coeficiente de silueta como métrica, tal y como muestra la \Cref{fig:silhouette_elbow} sugiere reducir el número de clusters a cuatro para evitar esta penalización por clusters muy juntos, pero se sigue obteniendo una puntuación baja al no tener ahora elementos tan similares dentro de cada cluster. \\
					 
 	\section{Búsqueda semántica de imágenes}
	 	
 		Una búsqueda semántica de imágenes consiste en encontrar imágenes dentro de un conjunto, en base a una descripción textual de estas. Aunque este no es uno de los aspectos a resolver para directamente optimizar el proceso de valoración de puntos de interés, esta funcionalidad podría ser útil para administradores y desarrolladores de la plataforma, o podría ofrecerse a los usuarios para poder buscar y navegar otros puntos de interés que visitar. Disponiendo de un transformer multimodal, es de interés demostrar que esta es otra de las aplicaciones que tiene. Como ejemplo, se van a tratar de encontrar las nueve imágenes que mejor se adecuen a cada uno de los siguientes textos. 
 		
 		\begin{itemize}
 			\item Parque infantil
 			\item Marcador de ruta
 			\item Cartel informativo
 			\item Fuentes, piscina, y elementos de agua
 			\item Murales, grafitis, dibujos, y arte
 		\end{itemize}
 		
 		Para poder comparar imágenes con textos, se necesita representar ambas informaciones en un espacio común donde tenga sentido hacer comparaciones entre estos. Para ello, se van a calcular los $\textbf{t}_i$ que serán los embeddings de cada uno de los textos en inglés, y se compararán con los $\textbf{p}_j$, que son los embeddings de cada una de las imágenes (calculados en las secciones previas). Una vez calculados todos los embeddings, se comparan tal y como se explicó en la \Cref{sec:transformers_multimodales}, calculando el coseno del ángulo que forman ambos embeddings. Cuanto más cercano a uno sea, más similares son los embeddings. Para realizar esto, al dataframe se añade una columna por cada uno de los textos, que contienen el valor de $\cos(\theta_k)$, donde $\theta_k = \widehat{\textbf{t}_i\textbf{p}_j}$. Ordenando el dataframe por cada una de estas columnas de manera descendente, se obtienen los siguientes grupos de imágenes. \\
 		
 		\begin{figure}[!h]
 			\centering
 			\includegraphics[width = .3\textwidth]{busqueda_parque}\hfill
 			\includegraphics[width = .3\textwidth]{busqueda_marcadores}\hfill
 			\includegraphics[width = .3\textwidth]{busqueda_carteles}\vspace{.7cm}
 			\includegraphics[width = .3\textwidth]{busqueda_agua}\hspace{.7cm}
 			\includegraphics[width = .3\textwidth]{busqueda_arte}
 			\caption{Búsqueda semántica de imágenes mediante texto}
 			\label{fig:busqueda_semantica}
 		\end{figure} 
 		
 		Se observa que el resultado es adecuado. En el caso de parques infantiles, carteles, y murales, se han obtenido imágenes que corresponden con lo que los textos indicaban. En la caso de los marcadores de ruta, se han obtenido los hitos del Camino de Santiago, cosa que no es incorrecta, pues se pueden considerar un subconjunto de estos. Lo que más llama la atención es el cartel que aparece para el caso de las fuentes. A primera vista podría parecer erróneo, pero al observar la imagen con mayor detalle (\Cref{fig:texto_embedding}), el título del cartel dice: ``\textit{Ruta de las fuentes de Montemayor y Fernán Núñez}''. El transformer ha sido capaz de interpretar el texto, y al aparecer en el título es posible que lo haya considerado importante, contribuyendo de manera significativa en el embedding de la imagen. \\
 		
 		\begin{figure}[!h]
 			\centering
 			\includegraphics[scale = .5]{cartel_busqueda}
 			\caption{Contribución del texto en una imagen a su embedding}
 			\label{fig:texto_embedding}
 		\end{figure} 
 		
 		El motivo de traducir los textos a inglés, es que en la documentación del modelo que se viene usando en este proyecto, indica que está preparado para recibir textos únicamente en inglés. Esto no sería un problema a la hora de llevar el proyecto a algo real, pues existen otros transformers especializados en traducciones. Por otro lado, tras realizar alguna prueba con los mismos textos en castellano, y observando el caso de la \Cref{fig:texto_embedding}, es probable que el corpus que utilizó Google para entrenar el transformer, contuviera información en castellano pero en menor medida, siendo su efectividad menor que en inglés. 
 		
	\section{Selección de una descripción para una imagen}
	
		Durante el proceso de revisión en Wayfarer, además de valorar propuestas, pueden aparecer ediciones de títulos o descripciones con el fin de corregir un error o encontrar un texto más preciso. La valoración de este cambio consiste en simplemente elegir el texto que mejor se adapta a la imagen mediante el criterio del valorador, tal y como se muestra en la \Cref{fig:edit_wayfarer}. \\
		
		\begin{figure}[!h]
			\centering
			\includegraphics[width = .4\textwidth, valign = c]{edit1}\hfill
			\includegraphics[width = .4\textwidth, valign = c]{edit2}
			\caption{Propuesta de edición en Wayfarer}
			\label{fig:edit_wayfarer}
		\end{figure}
		
		Esto puede entenderse como el mismo problema de la búsqueda semántica de imágenes, pero intercambiando los textos por imágenes y viceversa. Para resolverlo, se propone calcular los embeddings de todos los textos y la imagen, calcular el coseno que forma cada posible pareja de imagen y texto, y seleccionar el texto de la pareja que maximice el valor del coseno. Para demostrar la eficacia del método, se ha seleccionado la imagen mostrada en la \Cref{fig:edit_descripcion} del dataset original, y se simula la situación en la que se tienen tres descripciones entre las que se encuentra la correcta. 
		
		\begin{itemize}
			\item Pista de baloncesto donde niños practican deporte. 
			\item Grafiti en el que aparecen dos personas con trajes regionales, y se lee el texto ``\textit{Los alcarreñitos}''. 
			\item Piscina pública. 
		\end{itemize}
		
		\begin{figure}[!h]
			\centering
			\includegraphics[scale = .4]{edit_descripcion}
			\caption{Imagen para la simulación de edición de descripción}
			\label{fig:edit_descripcion}
		\end{figure}
		
		Al calcular los tres cosenos, se obtienen los valores $0.03$, $0.29$, y $0$. En el caso de la primera y la tercera descripción, indica que los embeddings de la imagen y el texto son perpendiculares, es decir, que no guardan relación alguna. Por otro lado, para la segunda descripción (la correcta), se obtiene un valor bastante más elevado en comparación con las otras dos. Esto indica que existe relación entre la información que el transformer ha obtenido de la imagen y la que ha obtenido del texto. Podría ser de esperar que si se está definiendo a la perfección la imagen, se obtuviera un coseno igual a 1, pero esto no es así, pues es posible que el transformer se esté fijando en otros detalles diferentes (o adicionales) a los que se ha fijado una persona al crear el texto y viceversa. Normalmente al trabajar con embeddings multimodales, las similitudes suelen ser más bajas que cuando estos provienen de una misma modalidad, afectando también la  alta dimensionalidad. 
	
		
					 
				